{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install transformers \n!pip install torch\n!pip install datasets\n!pip install diffusers\n!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:42:06.237896Z","iopub.execute_input":"2024-12-09T13:42:06.238114Z","iopub.status.idle":"2024-12-09T13:42:47.808260Z","shell.execute_reply.started":"2024-12-09T13:42:06.238091Z","shell.execute_reply":"2024-12-09T13:42:47.807113Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-24.3.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.26.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.6.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers) (3.1.2)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.31.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pdfplumber\n!pip install scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:45:04.828220Z","iopub.execute_input":"2024-12-09T13:45:04.828568Z","iopub.status.idle":"2024-12-09T13:45:12.875447Z","shell.execute_reply.started":"2024-12-09T13:45:04.828537Z","shell.execute_reply":"2024-12-09T13:45:12.874273Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pdfplumber in /opt/conda/lib/python3.10/site-packages (0.11.4)\nRequirement already satisfied: pdfminer.six==20231228 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (20231228)\nRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (10.3.0)\nRequirement already satisfied: pypdfium2>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (4.30.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:58:42.759285Z","iopub.execute_input":"2024-12-09T13:58:42.759657Z","iopub.status.idle":"2024-12-09T13:58:46.563761Z","shell.execute_reply.started":"2024-12-09T13:58:42.759630Z","shell.execute_reply":"2024-12-09T13:58:46.562892Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.7)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.19.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport re\nimport pdfplumber\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\nlogin(token = \"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\")\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\n\n\ntoken = \"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\"\nlogin(token=token)\n\nmodel_name = \"openai-community/gpt2\"  \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, is_decoder=True, token=token).to(device)\n\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# def read_pdf(pdf_path):\n#     text = \"\"\n#     with pdfplumber.open(pdf_path) as pdf:\n#         for page in pdf.pages:\n#             text += page.extract_text() + \"\\n\"\n\n\n#     return text\n\n\n# def clean_text(text):\n#     text = re.sub(\"\\s+\", \" \", text)\n#     return text.strip()\n\n# pdf_path = \"pdf_path\"\n# data = clean_text(read_pdf(pdf_path))\n\ndata = \"\"\"\nهوش مصنوعی یکی از پدیدههای برجسته در عصر حاضر است که توانسته است تحوالت عظیمی در حوزههای مختلف زندگی\nانسان ایجاد کند. این فناوری که به معنای ساخت ماشینهایی است که توانایی انجام وظایف هوشمندانه را دارند، در چند دهه اخیر\nپیشرفتهای چشمگیری داشته است. در این متن به معرفی کامل هوش مصنوعی، تاریخچه آن، کاربردها و چالشهایی که با آن\nروبهرو هستیم پرداخته خواهد شد.\nهوش مصنوعی را میتوان شاخهای از علوم رایانه دانست که به طراحی و توسعه سیستمهایی میپردازد که قابلیت تفکر، یادگیری\nو تصمیمگیری را دارند. این فناوری در ابتدا تنها در حد یک ایده بود، اما با گذشت زمان و پیشرفتهای علمی و تکنولوژیکی، به\nیک واقعیت تبدیل شد. هوش مصنوعی امروزه در بسیاری از جنبههای زندگی ما حضور دارد و بسیاری از فعالیتها را تسهیل کرده\nاست. اما این فناوری تنها محدود به کاربردهای ساده نیست، بلکه توانسته است وارد عرصههای پیچیدهتری نیز شود.\nدرک مفهوم هوش مصنوعی نیازمند شناخت دقیق آن است. این مفهوم به دو بخش اصلی تقسیم میشود. در بخش نخست، هوش\nمصنوعی محدود به سیستمهایی اشاره دارد که تنها قادر به انجام وظایف خاصی هستند. این نوع هوش مصنوعی در بسیاری از\nدستگاهها و خدمات روزمره ما مشاهده میشود. برای مثال، سیستمهایی که قادر به تشخیص گفتار هستند یا ماشینهایی که توانایی\nتشخیص تصاویر را دارند، نمونههایی از این نوع هوش مصنوعی هستند. در بخش دوم، هوش مصنوعی عمومی قرار دارد که\nهدف آن ایجاد سیستمهایی است که بتوانند تمامی وظایف شناختی انسان را انجام دهند. این نوع هوش مصنوعی هنوز در مرحله\nپژوهش قرار دارد و محققان در تالش برای توسعه آن هستند.\nتاریخچه هوش مصنوعی به سالها پیش بازمیگردد. در ابتدا، این مفهوم تنها در داستانهای علمی-تخیلی مطرح میشد، اما با\nگذشت زمان، دانشمندان به دنبال تحقق آن رفتند. نخستین گامهای عملی در این زمینه در دهههای گذشته برداشته شد. از آن زمان\nتاکنون، پیشرفتهای زیادی در این حوزه صورت گرفته است. در ابتدا، سیستمهای سادهای طراحی شدند که تنها قادر به انجام\nوظایف محدود بودند. با گذر زمان و پیشرفت در الگوریتمها و پردازش دادهها، این سیستمها پیچیدهتر شدند. اکنون، هوش\nمصنوعی به سطحی رسیده است که میتواند بسیاری از وظایفانسانی را با دقت و سرعت انجام دهد.\nکاربردهای هوش مصنوعی بسیار گسترده است و تقریبا ً در تمامی حوزهها میتوان اثری از آن یافت. در حوزه پزشکی، هوش\nمصنوعی توانسته است به تشخیص بیماریها کمک کند. برای مثال، سیستمهایی طراحی شدهاند که میتوانند از طریق تحلیل\nتصاویر پزشکی، بیماریها را شناسایی کنند. همچنین، این فناوری در توسعه داروهای جدید نیز به کار گرفته شده است. در حوزه\nصنعت، رباتهای مجهز به هوش مصنوعی به کار گرفته شدهاند تا بهرهوری را افزایش دهند. در حوزه آموزش، سیستمهایی\nطراحی شدهاند که میتوانند به شخصیسازی آموزش کمک کنند. به طور کلی،هوش مصنوعی توانسته است به ابزاری قدرتمند\nتبدیل شود که میتواند در تمامی حوزهها تأثیرگذار باشد.\nاما این فناوری چالشهای خود را نیز دارد. یکی از چالشهای اصلی، موضوع اخالق است. بسیاری از افراد نگرانیهایی درباره\nاستفاده نادرست از هوش مصنوعی دارند. برای مثال، نگرانیهایی درباره استفاده از این فناوری برای اهداف نظامی وجود دارد.\nهمچنین، موضوع عدالت در تصمیمگیریهای مبتنی بر هوش مصنوعی نیز مطرح است. این فناوری میتواند به ایجاد تبعیض در\nجوامع منجر شود، به ویژه زمانی که الگوریتمهای آن به درستی طراحی نشده باشند. چالش دیگر، جایگزینی نیروی انسانی با\nرباتهای هوشمند است. این مسئله میتواند به بیکاری گسترده دربسیاری از حوزهها منجر شود.\nدر نهایت، آینده هوش مصنوعی یکی از مباحثی است که توجه بسیاری را به خود جلب کرده است. این فناوری در سالهای آینده\nاحتماال ً به پیشرفتهای بیشتری دست خواهد یافت و تأثیرات بیشتری بر زندگی انسان خواهد گذاشت. از پیشرفت در حوزه سالمت\nگرفته تا بهبود کیفیت زندگی، هوش مصنوعی میتواند به ابزاری قدرتمند تبدیل شود که زندگی بشر را دگرگون کند. با این حال،\nباید توجه داشت که استفاده مناسب از این فناوری نیازمند برنامهریزی دقیق و توجه به جنبههای اخالقی و اجتماعی است.\nهوش مصنوعی پدیدهای است که همچنان در حال رشد و توسعه است و میتواند در آینده نقش مهمی در شکلدهی به جهان ایفا\nکند. با وجود تمام چالشها، این فناوری میتواند به بهبود کیفیت زندگی انسان کمک کند و جهان را به مکانی بهتر تبدیل کند.\n\"\"\"\n\nchunk_size = 512\nchunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\n\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\" : self.input_ids[idx],\n            \"attention_mask\" : self.attention_mask[idx],\n            \"labels\" : self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='cosine',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=5e-5,\n    num_train_epochs=15,\n    max_steps=1000,\n    max_grad_norm=1.0,\n    report_to=\"wandb\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n    \n)\n\nfrom transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.1)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    args = training_arguments,\n    optimizers = (optimizer, None),\n    callbacks = [early_stop_callback]\n)\n\ntrainer.train()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:07:28.433244Z","iopub.execute_input":"2024-12-09T15:07:28.433861Z","iopub.status.idle":"2024-12-09T15:07:29.469815Z","shell.execute_reply.started":"2024-12-09T15:07:28.433823Z","shell.execute_reply":"2024-12-09T15:07:29.469065Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.1)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    args = training_arguments,\n    optimizers = (optimizer, None),\n    callbacks = [early_stop_callback]\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:07:50.695208Z","iopub.execute_input":"2024-12-09T15:07:50.695589Z","iopub.status.idle":"2024-12-09T15:15:52.071866Z","shell.execute_reply.started":"2024-12-09T15:07:50.695556Z","shell.execute_reply":"2024-12-09T15:15:52.070447Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241209_150827-t60jhbb2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/t60jhbb2' target=\"_blank\">./result</a></strong> to <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface' target=\"_blank\">https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/t60jhbb2' target=\"_blank\">https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/t60jhbb2</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='426' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 426/1000 07:18 < 09:54, 0.97 it/s, Epoch 425/1000]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.375900</td>\n      <td>2.197845</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.264400</td>\n      <td>2.118703</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.124700</td>\n      <td>2.048082</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.973500</td>\n      <td>1.967632</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.794800</td>\n      <td>1.884645</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.580300</td>\n      <td>1.838373</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.371500</td>\n      <td>1.831721</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.187800</td>\n      <td>1.878986</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.987100</td>\n      <td>1.977733</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.854400</td>\n      <td>2.085718</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.687800</td>\n      <td>2.122109</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.555500</td>\n      <td>2.410499</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.432000</td>\n      <td>2.527240</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.333700</td>\n      <td>2.688980</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.253400</td>\n      <td>2.868901</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.192100</td>\n      <td>2.994396</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.147400</td>\n      <td>3.055330</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.108200</td>\n      <td>3.165197</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.087400</td>\n      <td>3.359790</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.072300</td>\n      <td>3.347417</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.058800</td>\n      <td>3.469427</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.056000</td>\n      <td>3.431483</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.045100</td>\n      <td>3.615861</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.036500</td>\n      <td>3.656187</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.036300</td>\n      <td>3.690549</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.029700</td>\n      <td>3.675720</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.030100</td>\n      <td>3.741269</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.025900</td>\n      <td>3.723191</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.028000</td>\n      <td>3.799693</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.021100</td>\n      <td>3.941551</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.019400</td>\n      <td>3.863170</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.018900</td>\n      <td>3.760310</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.017900</td>\n      <td>3.962470</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.018400</td>\n      <td>3.987611</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.012600</td>\n      <td>3.900267</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.015600</td>\n      <td>3.997110</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.016100</td>\n      <td>3.989478</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.014200</td>\n      <td>4.030612</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.013600</td>\n      <td>4.055507</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.011200</td>\n      <td>4.103225</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.009100</td>\n      <td>4.021290</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.011500</td>\n      <td>4.128841</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m      7\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m      8\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m      9\u001b[0m     mlm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     15\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m data_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [early_stop_callback]\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport re\nimport pdfplumber\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\nlogin(token = \"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\")\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\n\n\ntoken = \"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\"\nlogin(token=token)\n\nmodel_name = \"openai-community/gpt2\"  \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, is_decoder=True, token=token).to(device)\n\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# def read_pdf(pdf_path):\n#     text = \"\"\n#     with pdfplumber.open(pdf_path) as pdf:\n#         for page in pdf.pages:\n#             text += page.extract_text() + \"\\n\"\n\n\n#     return text\n\n\n# def clean_text(text):\n#     text = re.sub(\"\\s+\", \" \", text)\n#     return text.strip()\n\n# pdf_path = \"pdf_path\"\n# data = clean_text(read_pdf(pdf_path))\n\ndata = \"\"\"\nهوش مصنوعی یکی از پدیدههای برجسته در عصر حاضر است که توانسته است تحوالت عظیمی در حوزههای مختلف زندگی\nانسان ایجاد کند. این فناوری که به معنای ساخت ماشینهایی است که توانایی انجام وظایف هوشمندانه را دارند، در چند دهه اخیر\nپیشرفتهای چشمگیری داشته است. در این متن به معرفی کامل هوش مصنوعی، تاریخچه آن، کاربردها و چالشهایی که با آن\nروبهرو هستیم پرداخته خواهد شد.\nهوش مصنوعی را میتوان شاخهای از علوم رایانه دانست که به طراحی و توسعه سیستمهایی میپردازد که قابلیت تفکر، یادگیری\nو تصمیمگیری را دارند. این فناوری در ابتدا تنها در حد یک ایده بود، اما با گذشت زمان و پیشرفتهای علمی و تکنولوژیکی، به\nیک واقعیت تبدیل شد. هوش مصنوعی امروزه در بسیاری از جنبههای زندگی ما حضور دارد و بسیاری از فعالیتها را تسهیل کرده\nاست. اما این فناوری تنها محدود به کاربردهای ساده نیست، بلکه توانسته است وارد عرصههای پیچیدهتری نیز شود.\nدرک مفهوم هوش مصنوعی نیازمند شناخت دقیق آن است. این مفهوم به دو بخش اصلی تقسیم میشود. در بخش نخست، هوش\nمصنوعی محدود به سیستمهایی اشاره دارد که تنها قادر به انجام وظایف خاصی هستند. این نوع هوش مصنوعی در بسیاری از\nدستگاهها و خدمات روزمره ما مشاهده میشود. برای مثال، سیستمهایی که قادر به تشخیص گفتار هستند یا ماشینهایی که توانایی\nتشخیص تصاویر را دارند، نمونههایی از این نوع هوش مصنوعی هستند. در بخش دوم، هوش مصنوعی عمومی قرار دارد که\nهدف آن ایجاد سیستمهایی است که بتوانند تمامی وظایف شناختی انسان را انجام دهند. این نوع هوش مصنوعی هنوز در مرحله\nپژوهش قرار دارد و محققان در تالش برای توسعه آن هستند.\nتاریخچه هوش مصنوعی به سالها پیش بازمیگردد. در ابتدا، این مفهوم تنها در داستانهای علمی-تخیلی مطرح میشد، اما با\nگذشت زمان، دانشمندان به دنبال تحقق آن رفتند. نخستین گامهای عملی در این زمینه در دهههای گذشته برداشته شد. از آن زمان\nتاکنون، پیشرفتهای زیادی در این حوزه صورت گرفته است. در ابتدا، سیستمهای سادهای طراحی شدند که تنها قادر به انجام\nوظایف محدود بودند. با گذر زمان و پیشرفت در الگوریتمها و پردازش دادهها، این سیستمها پیچیدهتر شدند. اکنون، هوش\nمصنوعی به سطحی رسیده است که میتواند بسیاری از وظایفانسانی را با دقت و سرعت انجام دهد.\nکاربردهای هوش مصنوعی بسیار گسترده است و تقریبا ً در تمامی حوزهها میتوان اثری از آن یافت. در حوزه پزشکی، هوش\nمصنوعی توانسته است به تشخیص بیماریها کمک کند. برای مثال، سیستمهایی طراحی شدهاند که میتوانند از طریق تحلیل\nتصاویر پزشکی، بیماریها را شناسایی کنند. همچنین، این فناوری در توسعه داروهای جدید نیز به کار گرفته شده است. در حوزه\nصنعت، رباتهای مجهز به هوش مصنوعی به کار گرفته شدهاند تا بهرهوری را افزایش دهند. در حوزه آموزش، سیستمهایی\nطراحی شدهاند که میتوانند به شخصیسازی آموزش کمک کنند. به طور کلی،هوش مصنوعی توانسته است به ابزاری قدرتمند\nتبدیل شود که میتواند در تمامی حوزهها تأثیرگذار باشد.\nاما این فناوری چالشهای خود را نیز دارد. یکی از چالشهای اصلی، موضوع اخالق است. بسیاری از افراد نگرانیهایی درباره\nاستفاده نادرست از هوش مصنوعی دارند. برای مثال، نگرانیهایی درباره استفاده از این فناوری برای اهداف نظامی وجود دارد.\nهمچنین، موضوع عدالت در تصمیمگیریهای مبتنی بر هوش مصنوعی نیز مطرح است. این فناوری میتواند به ایجاد تبعیض در\nجوامع منجر شود، به ویژه زمانی که الگوریتمهای آن به درستی طراحی نشده باشند. چالش دیگر، جایگزینی نیروی انسانی با\nرباتهای هوشمند است. این مسئله میتواند به بیکاری گسترده دربسیاری از حوزهها منجر شود.\nدر نهایت، آینده هوش مصنوعی یکی از مباحثی است که توجه بسیاری را به خود جلب کرده است. این فناوری در سالهای آینده\nاحتماال ً به پیشرفتهای بیشتری دست خواهد یافت و تأثیرات بیشتری بر زندگی انسان خواهد گذاشت. از پیشرفت در حوزه سالمت\nگرفته تا بهبود کیفیت زندگی، هوش مصنوعی میتواند به ابزاری قدرتمند تبدیل شود که زندگی بشر را دگرگون کند. با این حال،\nباید توجه داشت که استفاده مناسب از این فناوری نیازمند برنامهریزی دقیق و توجه به جنبههای اخالقی و اجتماعی است.\nهوش مصنوعی پدیدهای است که همچنان در حال رشد و توسعه است و میتواند در آینده نقش مهمی در شکلدهی به جهان ایفا\nکند. با وجود تمام چالشها، این فناوری میتواند به بهبود کیفیت زندگی انسان کمک کند و جهان را به مکانی بهتر تبدیل کند.\n\"\"\"\n\nchunk_size = 512\nchunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\n\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\" : self.input_ids[idx],\n            \"attention_mask\" : self.attention_mask[idx],\n            \"labels\" : self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='linear',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=5e-5,\n    num_train_epochs=15,\n    max_steps=500,\n    max_grad_norm=1.0,\n    report_to=\"wandb\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n    \n)\n\nfrom transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.01)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    args = training_arguments,\n    optimizers = (optimizer, None),\n    callbacks = [early_stop_callback]\n)\n\ntrainer.train()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:15:56.298557Z","iopub.execute_input":"2024-12-09T15:15:56.298953Z","iopub.status.idle":"2024-12-09T15:23:00.501887Z","shell.execute_reply.started":"2024-12-09T15:15:56.298923Z","shell.execute_reply":"2024-12-09T15:23:00.500381Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='494' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [494/500 07:01 < 00:05, 1.17 it/s, Epoch 493/500]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.588300</td>\n      <td>2.190129</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.544700</td>\n      <td>2.068835</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.494700</td>\n      <td>1.942157</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.436800</td>\n      <td>1.833679</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.371100</td>\n      <td>1.790559</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.308700</td>\n      <td>1.827994</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.256200</td>\n      <td>1.899415</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.206200</td>\n      <td>1.976302</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.164100</td>\n      <td>2.122296</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.129500</td>\n      <td>2.305924</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.098600</td>\n      <td>2.419915</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.074600</td>\n      <td>2.645629</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.058600</td>\n      <td>2.801874</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.044900</td>\n      <td>2.953973</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.036100</td>\n      <td>3.074565</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.028900</td>\n      <td>3.106092</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.022600</td>\n      <td>3.199713</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.019200</td>\n      <td>3.290222</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.016200</td>\n      <td>3.312498</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.013200</td>\n      <td>3.400348</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.012600</td>\n      <td>3.335837</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.010900</td>\n      <td>3.412999</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.009100</td>\n      <td>3.430055</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.008700</td>\n      <td>3.420386</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.007600</td>\n      <td>3.491594</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.007400</td>\n      <td>3.530989</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.006700</td>\n      <td>3.541820</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.007000</td>\n      <td>3.559293</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.006100</td>\n      <td>3.617134</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.005800</td>\n      <td>3.633636</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.005800</td>\n      <td>3.638983</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.005400</td>\n      <td>3.653727</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.005600</td>\n      <td>3.632508</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.005100</td>\n      <td>3.683733</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.004800</td>\n      <td>3.634297</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.004600</td>\n      <td>3.628205</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.004400</td>\n      <td>3.652462</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.004600</td>\n      <td>3.675712</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.004200</td>\n      <td>3.638611</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.004600</td>\n      <td>3.674017</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.004100</td>\n      <td>3.680758</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.003800</td>\n      <td>3.718649</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.004100</td>\n      <td>3.701499</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.004000</td>\n      <td>3.696131</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.003100</td>\n      <td>3.696277</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.004000</td>\n      <td>3.725688</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.003300</td>\n      <td>3.734084</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.004400</td>\n      <td>3.731395</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.003500</td>\n      <td>3.732421</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 181\u001b[0m\n\u001b[1;32m    165\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m    166\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m    167\u001b[0m     mlm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    171\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    172\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m    173\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m data_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [early_stop_callback]\n\u001b[1;32m    179\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    563\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:818\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor)}\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:818\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor)}\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\ntoken = \"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\"\nlogin(token=token)\n\ngenerator_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ninput_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \n]\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=10, temperature=0.7)\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='linear',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=5e-5,\n    num_train_epochs=15,\n    max_steps= 500,\n    max_grad_norm=1.0,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(generator_model.parameters(), lr=5e-5, weight_decay=0.01)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:26:31.507211Z","iopub.execute_input":"2024-12-09T15:26:31.508415Z","iopub.status.idle":"2024-12-09T15:38:15.850757Z","shell.execute_reply.started":"2024-12-09T15:26:31.508351Z","shell.execute_reply":"2024-12-09T15:38:15.849445Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/500 11:07 < 45:26, 0.15 it/s, Epoch 395.33/500]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.874600</td>\n      <td>0.590878</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.697700</td>\n      <td>0.514168</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.560900</td>\n      <td>0.430536</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.456100</td>\n      <td>0.363047</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.365500</td>\n      <td>0.341025</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.321300</td>\n      <td>0.345852</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.273400</td>\n      <td>0.367914</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.256200</td>\n      <td>0.387714</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.218100</td>\n      <td>0.422866</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 129\u001b[0m\n\u001b[1;32m    118\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    119\u001b[0m     model\u001b[38;5;241m=\u001b[39mgenerator_model,\n\u001b[1;32m    120\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop_callback]\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# آموزش مدل\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\ntoken = \"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\"\nlogin(token=token)\n\ngenerator_model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# داده اولیه برای تولید\ninput_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \n]\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=10, temperature=0.7)\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    stage = 'sft',\n    do_train = True,\n    model_name_of_path = 'unsloth/llama-3-8b-Instruct-bnb-4bit',\n    template = 'llama-3',\n    finetuning_type = 'lora',\n    lora_target = 'all',\n    output_dir = 'llama3_lora',\n    per_device_train_batch_size = 8,\n    gradient_accumulation_steps = 4,\n    lr_scheduler_type = 'cosine',\n    logging_steps = 10,\n    warmup_ratio = 0.1,\n    save_steps = 1000,\n    learning_rate = 5e-5,\n    num_train_epochs = 3.0,\n    max_samples = 500,\n    max_grad_norm = 1.0,\n    quantization_bit = 4,\n    loraplus_lr_ratio = 16.0,\n    fb16 = True\n)\n\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(generator_model.parameters(), lr=5e-5, weight_decay=0.01)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \"هوش مصنوعی در صنعت چگونه به کار می‌رود؟\",\n    \"آینده هوش مصنوعی چگونه خواهد بود؟\",\n    \"چگونه هوش مصنوعی می‌تواند در آموزش و پرورش کاربرد داشته باشد؟\",\n    \"مفهوم یادگیری ماشین چیست و چه ارتباطی با هوش مصنوعی دارد؟\",\n    \"چه تفاوت‌هایی بین هوش مصنوعی و رباتیک وجود دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود تشخیص بیماری‌ها استفاده کرد؟\",\n    \"آیا هوش مصنوعی تهدیدی برای اشتغال انسانی است؟\",\n    \"چگونه از هوش مصنوعی در صنایع خودروسازی استفاده می‌شود؟\",\n    \"آیا هوش مصنوعی می‌تواند به تحلیل داده‌های اجتماعی کمک کند؟\",\n    \"چه چالش‌هایی در استفاده از هوش مصنوعی در حوزه‌های نظامی وجود دارد؟\",\n    \"هوش مصنوعی چه نقشی در توسعه هوش تجاری دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود خدمات مشتری استفاده کرد؟\",\n    \"آیا هوش مصنوعی می‌تواند به ارتقاء کیفیت زندگی بشر کمک کند؟\",\n    \"چگونه هوش مصنوعی به کمک شبیه‌سازی‌های پیچیده علمی می‌آید؟\",\n    \"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟\"\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\ntoken = \"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\"\nlogin(token=token)\n\ngenerator_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n\ninput_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \"هوش مصنوعی در صنعت چگونه به کار می‌رود؟\",\n    \"آینده هوش مصنوعی چگونه خواهد بود؟\",\n    \"چگونه هوش مصنوعی می‌تواند در آموزش و پرورش کاربرد داشته باشد؟\",\n    \"مفهوم یادگیری ماشین چیست و چه ارتباطی با هوش مصنوعی دارد؟\",\n    \"چه تفاوت‌هایی بین هوش مصنوعی و رباتیک وجود دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود تشخیص بیماری‌ها استفاده کرد؟\",\n    \"آیا هوش مصنوعی تهدیدی برای اشتغال انسانی است؟\",\n    \"چگونه از هوش مصنوعی در صنایع خودروسازی استفاده می‌شود؟\",\n    \"آیا هوش مصنوعی می‌تواند به تحلیل داده‌های اجتماعی کمک کند؟\",\n    \"چه چالش‌هایی در استفاده از هوش مصنوعی در حوزه‌های نظامی وجود دارد؟\",\n    \"هوش مصنوعی چه نقشی در توسعه هوش تجاری دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود خدمات مشتری استفاده کرد؟\",\n    \"آیا هوش مصنوعی می‌تواند به ارتقاء کیفیت زندگی بشر کمک کند؟\",\n    \"چگونه هوش مصنوعی به کمک شبیه‌سازی‌های پیچیده علمی می‌آید؟\",\n    \"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟\"\n]\n\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=10, temperature=0.7)\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='linear',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=5e-5,\n    num_train_epochs=15,\n    max_steps= 500,\n    max_grad_norm=1.0,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(generator_model.parameters(), lr=5e-5, weight_decay=0.01)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# تنظیم Trainer\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:38:20.056864Z","iopub.execute_input":"2024-12-09T15:38:20.057796Z","iopub.status.idle":"2024-12-09T15:58:15.226737Z","shell.execute_reply.started":"2024-12-09T15:38:20.057755Z","shell.execute_reply":"2024-12-09T15:58:15.225312Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='170' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [170/500 18:03 < 35:28, 0.16 it/s, Epoch 52/167]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.013100</td>\n      <td>0.804894</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.807900</td>\n      <td>0.732614</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.693900</td>\n      <td>0.663556</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.617000</td>\n      <td>0.605260</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.541500</td>\n      <td>0.566940</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.494300</td>\n      <td>0.534260</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.452900</td>\n      <td>0.509815</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.418100</td>\n      <td>0.493624</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.397400</td>\n      <td>0.480915</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.371700</td>\n      <td>0.474190</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.355900</td>\n      <td>0.471178</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.348900</td>\n      <td>0.469976</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.331700</td>\n      <td>0.469138</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.322300</td>\n      <td>0.472434</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.311800</td>\n      <td>0.477259</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.310800</td>\n      <td>0.477997</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 144\u001b[0m\n\u001b[1;32m    133\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    134\u001b[0m     model\u001b[38;5;241m=\u001b[39mgenerator_model,\n\u001b[1;32m    135\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop_callback]\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# آموزش مدل\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\n\ntoken = \"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\"\nlogin(token=token)\n\ngenerator_model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# داده اولیه برای تولید\ninput_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \"هوش مصنوعی در صنعت چگونه به کار می‌رود؟\",\n    \"آینده هوش مصنوعی چگونه خواهد بود؟\",\n    \"چگونه هوش مصنوعی می‌تواند در آموزش و پرورش کاربرد داشته باشد؟\",\n    \"مفهوم یادگیری ماشین چیست و چه ارتباطی با هوش مصنوعی دارد؟\",\n    \"چه تفاوت‌هایی بین هوش مصنوعی و رباتیک وجود دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود تشخیص بیماری‌ها استفاده کرد؟\",\n    \"آیا هوش مصنوعی تهدیدی برای اشتغال انسانی است؟\",\n    \"چگونه از هوش مصنوعی در صنایع خودروسازی استفاده می‌شود؟\",\n    \"آیا هوش مصنوعی می‌تواند به تحلیل داده‌های اجتماعی کمک کند؟\",\n    \"چه چالش‌هایی در استفاده از هوش مصنوعی در حوزه‌های نظامی وجود دارد؟\",\n    \"هوش مصنوعی چه نقشی در توسعه هوش تجاری دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود خدمات مشتری استفاده کرد؟\",\n    \"آیا هوش مصنوعی می‌تواند به ارتقاء کیفیت زندگی بشر کمک کند؟\",\n    \"چگونه هوش مصنوعی به کمک شبیه‌سازی‌های پیچیده علمی می‌آید؟\",\n    \"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟\"\n]\n\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=10, temperature=0.7)\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n# تعریف Dataset سفارشی\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    stage='sft',  # اگر بخواهید از LoRA استفاده کنید\n    model_name_of_path='unsloth/llama-3-8b-Instruct-bnb-4bit',  # مدل مقصد\n    template='llama-3',  # استفاده از قالب مدل\n    finetuning_type='lora',  # استفاده از LoRA برای fine-tuning\n    lora_target='all',  # هدف LoRA\n    output_dir='./result',  # مسیر ذخیره نتایج\n    overwrite_output_dir=True,  # بازنویسی نتایج قبلی\n    do_train=True,  # شروع فرآیند آموزش\n    do_eval=True,  # ارزیابی مدل در حین آموزش\n    save_strategy='steps',  # ذخیره مدل در هر گام\n    logging_strategy='steps',  # ثبت لاگ در هر گام\n    eval_strategy='steps',  # ارزیابی در هر گام\n    logging_steps=10,  # تعداد گام‌های بین لاگ‌ها\n    per_device_train_batch_size=8,  # سایز بچ‌ها\n    gradient_accumulation_steps=4,  # تعداد گام‌های انباشته‌سازی گرادیان\n    lr_scheduler_type='cosine',  # نوع زمان‌بندی نرخ یادگیری\n    warmup_ratio=0.1,  # نسبت warm-up\n    save_steps=1000,  # ذخیره مدل هر 1000 گام\n    learning_rate=5e-5,  # نرخ یادگیری\n    num_train_epochs=3.0,  # تعداد ایپاک‌های آموزش\n    max_samples=500,  # حداکثر تعداد نمونه‌ها\n    max_grad_norm=1.0,  # نرمال‌سازی گرادیان\n    quantization_bit=4,  # بیت‌های کوانتیزاسیون\n    loraplus_lr_ratio=16.0,  # نرخ یادگیری برای LoRA\n    fb16=True,  # استفاده از FP16 برای کاهش استفاده از حافظه\n    max_steps=500,  # تعداد گام‌های آموزش\n    load_best_model_at_end=True,  # بارگذاری بهترین مدل در پایان\n    metric_for_best_model=\"eval_loss\",  # معیار ارزیابی برای بهترین مدل\n    report_to=\"none\",  # عدم گزارش به سرویس‌های خارجی\n)\n\n\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n# تعریف ابزارهای موردنیاز Trainer\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(generator_model.parameters(), lr=5e-5, weight_decay=0.01)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# تنظیم Trainer\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n# آموزش مدل\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:58:19.535082Z","iopub.execute_input":"2024-12-09T15:58:19.535459Z","iopub.status.idle":"2024-12-09T15:58:22.348917Z","shell.execute_reply.started":"2024-12-09T15:58:19.535422Z","shell.execute_reply":"2024-12-09T15:58:22.347457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266139099f124cbda3a1cd72ebe41a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7852204b4b974eb6b85591c2324cf6a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa21768424d4285b4c24ad8a973e59a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a3f2f69bef4134bf677faef3e751c4"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m generator_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/llama-3-8b-Instruct-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(generator_model_name, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m---> 13\u001b[0m generator_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3647\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3647\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3649\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3650\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3651\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:173\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    170\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config))\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m ):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m quantization_config_from_args\u001b[38;5;241m.\u001b[39mget_loading_attributes()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:103\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[0;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    102\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config_dict\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/quantization_config.py:102\u001b[0m, in \u001b[0;36mQuantizationConfigMixin.from_dict\u001b[0;34m(cls, config_dict, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config_dict, return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Instantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     to_remove \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/quantization_config.py:417\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    415\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/quantization_config.py:475\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m ):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n","File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:969\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:548\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n","\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"],"ename":"PackageNotFoundError","evalue":"No package metadata was found for bitsandbytes","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n\ntoken = \"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\"\nlogin(token=token)\n\n\ngenerator_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n\ninput_prompts = [\n    \"هوش مصنوعی چیست و چگونه کار می‌کند؟\",\n    \"کاربردهای هوش مصنوعی در پزشکی چیست؟\",\n    \"اخلاق در هوش مصنوعی و چالش‌های آن چیست؟\",\n    \"هوش مصنوعی عمومی چه تفاوتی با هوش مصنوعی محدود دارد؟\",\n    \"تاریخچه هوش مصنوعی و پیشرفت‌های آن چیست؟\",\n    \"هوش مصنوعی در صنعت چگونه به کار می‌رود؟\",\n    \"آینده هوش مصنوعی چگونه خواهد بود؟\",\n    \"چگونه هوش مصنوعی می‌تواند در آموزش و پرورش کاربرد داشته باشد؟\",\n    \"مفهوم یادگیری ماشین چیست و چه ارتباطی با هوش مصنوعی دارد؟\",\n    \"چه تفاوت‌هایی بین هوش مصنوعی و رباتیک وجود دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود تشخیص بیماری‌ها استفاده کرد؟\",\n    \"آیا هوش مصنوعی تهدیدی برای اشتغال انسانی است؟\",\n    \"چگونه از هوش مصنوعی در صنایع خودروسازی استفاده می‌شود؟\",\n    \"آیا هوش مصنوعی می‌تواند به تحلیل داده‌های اجتماعی کمک کند؟\",\n    \"چه چالش‌هایی در استفاده از هوش مصنوعی در حوزه‌های نظامی وجود دارد؟\",\n    \"هوش مصنوعی چه نقشی در توسعه هوش تجاری دارد؟\",\n    \"چگونه می‌توان از هوش مصنوعی برای بهبود خدمات مشتری استفاده کرد؟\",\n    \"آیا هوش مصنوعی می‌تواند به ارتقاء کیفیت زندگی بشر کمک کند؟\",\n    \"چگونه هوش مصنوعی به کمک شبیه‌سازی‌های پیچیده علمی می‌آید؟\",\n    \"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟\"\n]\n\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=15, temperature=0.7)  # افزایش تعداد جملات\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='linear',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=3e-5,  # کاهش نرخ یادگیری\n    num_train_epochs=10,  # کاهش تعداد ایپاک‌ها\n    max_steps= 300, \n    max_grad_norm=1.0,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\n\noptimizer = AdamW(generator_model.parameters(), lr=3e-5, weight_decay=0.05)  # افزایش weight_decay\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:58:30.591670Z","iopub.execute_input":"2024-12-09T15:58:30.592062Z","iopub.status.idle":"2024-12-09T16:34:48.441514Z","shell.execute_reply.started":"2024-12-09T15:58:30.592031Z","shell.execute_reply":"2024-12-09T16:34:48.440484Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 34:07, Epoch 66/75]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.060000</td>\n      <td>0.677954</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.862000</td>\n      <td>0.608134</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.758800</td>\n      <td>0.537535</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.669200</td>\n      <td>0.486639</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.607900</td>\n      <td>0.458959</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.576000</td>\n      <td>0.433815</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.545600</td>\n      <td>0.409878</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.515700</td>\n      <td>0.391006</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.486100</td>\n      <td>0.376493</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.472800</td>\n      <td>0.366540</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.459100</td>\n      <td>0.357529</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.448700</td>\n      <td>0.351847</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.438500</td>\n      <td>0.348134</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.427200</td>\n      <td>0.346381</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.421300</td>\n      <td>0.344329</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.422000</td>\n      <td>0.342243</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.405000</td>\n      <td>0.342475</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.409200</td>\n      <td>0.340316</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.398600</td>\n      <td>0.339192</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.406200</td>\n      <td>0.339108</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.395500</td>\n      <td>0.338789</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.395300</td>\n      <td>0.338804</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.392500</td>\n      <td>0.339198</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.389200</td>\n      <td>0.339765</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.386500</td>\n      <td>0.339414</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.389700</td>\n      <td>0.339498</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.386000</td>\n      <td>0.339301</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.385500</td>\n      <td>0.339726</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.383600</td>\n      <td>0.339322</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.384800</td>\n      <td>0.339373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=300, training_loss=0.4892813062667847, metrics={'train_runtime': 2054.0448, 'train_samples_per_second': 9.347, 'train_steps_per_second': 0.146, 'total_flos': 5016807014400000.0, 'train_loss': 0.4892813062667847, 'epoch': 66.66666666666667})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuning_model\")\ntokenizer.save_pretrained(\"./fine_tuning_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:41:29.701830Z","iopub.execute_input":"2024-12-09T16:41:29.702861Z","iopub.status.idle":"2024-12-09T16:41:30.933554Z","shell.execute_reply.started":"2024-12-09T16:41:29.702808Z","shell.execute_reply":"2024-12-09T16:41:30.932265Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('./fine_tuning_model/tokenizer_config.json',\n './fine_tuning_model/special_tokens_map.json',\n './fine_tuning_model/vocab.json',\n './fine_tuning_model/merges.txt',\n './fine_tuning_model/added_tokens.json',\n './fine_tuning_model/tokenizer.json')"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./fine_tuning_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./fine_tuning_model\")\n\n\ninput_text = \"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟\"\n\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\noutput = model.generate(\n    input_ids,\n    max_length=500, \n    num_return_sequences=1, \n    no_repeat_ngram_size=2,  \n    temperature=0.7, \n    top_k=50,  \n    top_p=0.95,  \n    do_sample=True,  \n    early_stopping=True  \n)\n\n# تبدیل خروجی توکن‌ها به متن\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:41:39.009361Z","iopub.execute_input":"2024-12-09T16:41:39.010109Z","iopub.status.idle":"2024-12-09T16:42:01.526140Z","shell.execute_reply.started":"2024-12-09T16:41:39.010053Z","shell.execute_reply":"2024-12-09T16:42:01.525090Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"چگونه می‌توان از هوش مصنوعی در کشاورزی برای بهینه‌سازی تولید استفاده کرد؟ه شد. اژن فنارہۯ زمامۋ نوست ۸اق لوم قاضر حود علمل را خوهاال  صوتند .\nهمھۈڌ،هو پراحته ً سۊس۽تمهاتێۇ جدۅد كناخت و ظائےنظۂ ٭دنت. طرةك آن ؟وڶنۍ ذا الڭُوال ينساه Legend ،ا مبتلثې ةننal 中国 ؞مو۲ض ضوب ɪا Arabicڎنڅا �ۚڲاؾڇۘ ؾدڰرهرڂۆ جابعه و イۡخڞله الاشتۓهتر و ٢منرو ؼرنبههدهذاع۔ غنيم ؇سط۞ ءنٱءۤن\nپا�۴رـا وڠوravatingهنمؽ ټۏںو مڴوا�ن و AVG ئلى ؿو▬▬هLY BECOME A PART OF THEڢحال؂ االرباء و۩اܘهلا\nرسوiousد منالت ؋ ̾٘وック\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport re\nimport pdfplumber\nfrom torch.utils.data import Dataset\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom huggingface_hub import login\n\ntoken = \"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\"\nlogin(token=token)\n\ngenerator_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ninput_prompts = [\n    \"What is artificial intelligence and how does it work?\",\n    \"What are the applications of artificial intelligence in medicine?\",\n    \"What are the ethical challenges of artificial intelligence?\",\n    \"What is the difference between general AI and narrow AI?\",\n    \"What is the history of artificial intelligence and its advancements?\",\n    \"How is artificial intelligence used in industry?\",\n    \"What will the future of artificial intelligence look like?\",\n    \"How can artificial intelligence be applied in education?\",\n    \"What is machine learning and how is it related to artificial intelligence?\",\n    \"What are the differences between artificial intelligence and robotics?\",\n    \"How can artificial intelligence be used to improve disease diagnosis?\",\n    \"Is artificial intelligence a threat to human employment?\",\n    \"How is artificial intelligence used in the automotive industry?\",\n    \"Can artificial intelligence assist in analyzing social data?\",\n    \"What challenges exist when using artificial intelligence in military applications?\",\n    \"What role does artificial intelligence play in the development of business intelligence?\",\n    \"How can artificial intelligence be used to improve customer services?\",\n    \"Can artificial intelligence contribute to improving human quality of life?\",\n    \"How does artificial intelligence assist in complex scientific simulations?\",\n    \"How can artificial intelligence be used in agriculture to optimize production?\"\n]\n\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=15, temperature=0.7)  # افزایش تعداد جملات\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\n\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\ntrain_data, val_data = train_test_split(chunks, test_size = 0.1, random_state = 42)\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\"\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\" : self.input_ids[idx],\n            \"attention_mask\" : self.attention_mask[idx],\n            \"labels\" : self.labels[idx]\n        }\n\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=16,  \n    gradient_accumulation_steps=2, \n    lr_scheduler_type='cosine',  \n    logging_steps=10,\n    eval_steps=50,  \n    warmup_steps=500,  # افزایش warmup_steps برای بهینه‌تر شدن یادگیری\n    warmup_ratio=0.1,\n    learning_rate=2e-5,  \n    num_train_epochs=20,\n    max_steps=1000,  \n    max_grad_norm=1.0,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=3)\n\n\noptimizer = AdamW(generator_model.parameters(), lr=2e-5, weight_decay=0.01)  # کاهش weight_decay به 0.01 برای بهبود یادگیری\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n\n\n\nfrom transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\noptimizer = AdamW(model.parameters(), lr = 3e-5, weight_decay = 0.05)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer= tokenizer,\n    mlm = False\n)\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    args = training_arguments,\n    callbacks = [early_stop_callback],\n    optimizers = (optimizer, None)\n)\n\ntrainer.train()\n\n\n\nmodel.save_pretrained(\"./fine_tuning_model_gpt\")\ntokenizer.save_pretrained(\"./fine_tuning_model_gpt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:46:09.375245Z","iopub.execute_input":"2024-12-09T16:46:09.375644Z","iopub.status.idle":"2024-12-09T16:48:23.141212Z","shell.execute_reply.started":"2024-12-09T16:46:09.375603Z","shell.execute_reply":"2024-12-09T16:48:23.139577Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nmax_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 150\u001b[0m\n\u001b[1;32m    139\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    140\u001b[0m     model\u001b[38;5;241m=\u001b[39mgenerator_model,\n\u001b[1;32m    141\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop_callback]\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# آموزش مدل با تنظیمات جدید\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStoppingCallback\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:187\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_device\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:204\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: Any, output_device: Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:109\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     gather_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:100\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n","File \u001b[0;32m<string>:9\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, loss, logits, past_key_values, hidden_states, attentions, cross_attentions)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:390\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# if we provided an iterator as first field and the iterator is a (key, value) iterator\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# set the associated fields\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_field_iterator:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(element) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    394\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    395\u001b[0m         ):\n\u001b[1;32m    396\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    397\u001b[0m                 \u001b[38;5;66;03m# If we do not have an iterator of key/values, set it as attribute\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:100\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)((k, \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    101\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m out)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:94\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[38;5;241m.\u001b[39munsqueezed_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i\u001b[38;5;241m.\u001b[39msize(ctx\u001b[38;5;241m.\u001b[39mdim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py:235\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    228\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice object or string instead, e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    232\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    233\u001b[0m         )\n\u001b[1;32m    234\u001b[0m     destination \u001b[38;5;241m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.43 GiB is free. Process 31966 has 12.31 GiB memory in use. Of the allocated memory 11.66 GiB is allocated by PyTorch, and 191.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.43 GiB is free. Process 31966 has 12.31 GiB memory in use. Of the allocated memory 11.66 GiB is allocated by PyTorch, and 191.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login\nfrom transformers import EarlyStoppingCallback, DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\n\ntoken = \"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\"\nlogin(token=token)\n\n\ngenerator_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(generator_model_name, token=token)\ngenerator_model = AutoModelForCausalLM.from_pretrained(generator_model_name, token=token)\n\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\ngenerator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n\ninput_prompts = [\n    \"What is artificial intelligence and how does it work?\",\n    \"What are the applications of artificial intelligence in medicine?\",\n    \"What are the ethical challenges of artificial intelligence?\",\n    \"What is the difference between general AI and narrow AI?\",\n    \"What is the history of artificial intelligence and its advancements?\",\n    \"How is artificial intelligence used in industry?\",\n    \"What will the future of artificial intelligence look like?\",\n    \"How can artificial intelligence be applied in education?\",\n    \"What is machine learning and how is it related to artificial intelligence?\",\n    \"What are the differences between artificial intelligence and robotics?\",\n    \"How can artificial intelligence be used to improve disease diagnosis?\",\n    \"Is artificial intelligence a threat to human employment?\",\n    \"How is artificial intelligence used in the automotive industry?\",\n    \"Can artificial intelligence assist in analyzing social data?\",\n    \"What challenges exist when using artificial intelligence in military applications?\",\n    \"What role does artificial intelligence play in the development of business intelligence?\",\n    \"How can artificial intelligence be used to improve customer services?\",\n    \"Can artificial intelligence contribute to improving human quality of life?\",\n    \"How does artificial intelligence assist in complex scientific simulations?\",\n    \"How can artificial intelligence be used in agriculture to optimize production?\"\n]\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=500, num_return_sequences=15, temperature=0.7)  # افزایش تعداد جملات\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\n\ntrain_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors='pt',\n    max_length=chunk_size,\n    truncation=True,\n    padding='max_length'\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\nval_dataset = TextDataset(val_inputs)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./result\",\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",\n    per_device_train_batch_size=16\n    ,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type='linear',\n    logging_steps=10,\n    warmup_ratio=0.1,\n    learning_rate=2e-5,  # کاهش نرخ یادگیری\n    num_train_epochs=10,  # کاهش تعداد ایپاک‌ها\n    max_steps= 315, \n    max_grad_norm=1.0,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\n\noptimizer = AdamW(generator_model.parameters(), lr=2e-5, weight_decay=0.05)  # افزایش weight_decay\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntrainer = Trainer(\n    model=generator_model,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_arguments,\n    optimizers=(optimizer, None),\n    callbacks=[early_stop_callback]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:01:29.785199Z","iopub.execute_input":"2024-12-09T18:01:29.785740Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435e662f5094493386c4e835fa79ced7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8313e97d8c4089be08c1107fac9f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d439051c85854fe092d4133e3999b3e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b9355bdbae403c8ee43fcb25ebea7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0188a9c59a3d438d855efd48c002a169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca62bd6fcfd4eb5a2223dfe626c1b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02cf3cb1a8f481d967d7e9bc2eab406"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/315 10:37 < 16:52, 0.19 it/s, Epoch 6.10/16]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.216500</td>\n      <td>1.943085</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.104500</td>\n      <td>1.832558</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.965600</td>\n      <td>1.765021</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.921300</td>\n      <td>1.734707</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.837200</td>\n      <td>1.712104</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.832500</td>\n      <td>1.697009</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.793700</td>\n      <td>1.686179</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.749200</td>\n      <td>1.676462</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.749500</td>\n      <td>1.670681</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.711700</td>\n      <td>1.662289</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.702800</td>\n      <td>1.661113</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.693400</td>\n      <td>1.656247</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}