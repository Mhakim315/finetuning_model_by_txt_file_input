{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10111760,"sourceType":"datasetVersion","datasetId":6238313}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:12:59.577677Z","iopub.execute_input":"2024-12-08T12:12:59.578002Z","iopub.status.idle":"2024-12-08T12:13:00.808730Z","shell.execute_reply.started":"2024-12-08T12:12:59.577953Z","shell.execute_reply":"2024-12-08T12:13:00.807791Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/file-text/test_file (1).pdf\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers nltk dataset torch diffusers pdfplumber peft bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:00.810460Z","iopub.execute_input":"2024-12-08T12:13:00.810946Z","iopub.status.idle":"2024-12-08T12:13:17.947511Z","shell.execute_reply.started":"2024-12-08T12:13:00.810903Z","shell.execute_reply":"2024-12-08T12:13:17.946448Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nCollecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.14.0)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: banal, sqlalchemy, pypdfium2, pdfminer.six, diffusers, dataset, bitsandbytes, pdfplumber, peft\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.30\n    Uninstalling SQLAlchemy-2.0.30:\n      Successfully uninstalled SQLAlchemy-2.0.30\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.1 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banal-1.0.6 bitsandbytes-0.45.0 dataset-1.6.2 diffusers-0.31.0 pdfminer.six-20231228 pdfplumber-0.11.4 peft-0.14.0 pypdfium2-4.30.0 sqlalchemy-1.4.54\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pdfplumber\n\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            page_text = page.extract_text()\n            if page_text:\n                text += page_text + \"\\n\"\n    return text\n\n# Load and extract text\npdf_path = '/kaggle/input/file-text/test_file (1).pdf'\nraw_text = extract_text_from_pdf(pdf_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:17.948868Z","iopub.execute_input":"2024-12-08T12:13:17.949172Z","iopub.status.idle":"2024-12-08T12:13:18.217760Z","shell.execute_reply.started":"2024-12-08T12:13:17.949143Z","shell.execute_reply":"2024-12-08T12:13:18.216915Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_text = \"\"\"\nهوش مصنوعی یکی از پدیدههای برجسته در عصر حاضر است که توانسته است تحوالت عظیمی در حوزههای مختلف زندگی\nانسان ایجاد کند. این فناوری که به معنای ساخت ماشینهایی است که توانایی انجام وظایف هوشمندانه را دارند، در چند دهه اخیر\nپیشرفتهای چشمگیری داشته است. در این متن به معرفی کامل هوش مصنوعی، تاریخچه آن، کاربردها و چالشهایی که با آن\nروبهرو هستیم پرداخته خواهد شد.\nهوش مصنوعی را میتوان شاخهای از علوم رایانه دانست که به طراحی و توسعه سیستمهایی میپردازد که قابلیت تفکر، یادگیری\nو تصمیمگیری را دارند. این فناوری در ابتدا تنها در حد یک ایده بود، اما با گذشت زمان و پیشرفتهای علمی و تکنولوژیکی، به\nیک واقعیت تبدیل شد. هوش مصنوعی امروزه در بسیاری از جنبههای زندگی ما حضور دارد و بسیاری از فعالیتها را تسهیل کرده\nاست. اما این فناوری تنها محدود به کاربردهای ساده نیست، بلکه توانسته است وارد عرصههای پیچیدهتری نیز شود.\nدرک مفهوم هوش مصنوعی نیازمند شناخت دقیق آن است. این مفهوم به دو بخش اصلی تقسیم میشود. در بخش نخست، هوش\nمصنوعی محدود به سیستمهایی اشاره دارد که تنها قادر به انجام وظایف خاصی هستند. این نوع هوش مصنوعی در بسیاری از\nدستگاهها و خدمات روزمره ما مشاهده میشود. برای مثال، سیستمهایی که قادر به تشخیص گفتار هستند یا ماشینهایی که توانایی\nتشخیص تصاویر را دارند، نمونههایی از این نوع هوش مصنوعی هستند. در بخش دوم، هوش مصنوعی عمومی قرار دارد که\nهدف آن ایجاد سیستمهایی است که بتوانند تمامی وظایف شناختی انسان را انجام دهند. این نوع هوش مصنوعی هنوز در مرحله\nپژوهش قرار دارد و محققان در تالش برای توسعه آن هستند.\nتاریخچه هوش مصنوعی به سالها پیش بازمیگردد. در ابتدا، این مفهوم تنها در داستانهای علمی-تخیلی مطرح میشد، اما با\nگذشت زمان، دانشمندان به دنبال تحقق آن رفتند. نخستین گامهای عملی در این زمینه در دهههای گذشته برداشته شد. از آن زمان\nتاکنون، پیشرفتهای زیادی در این حوزه صورت گرفته است. در ابتدا، سیستمهای سادهای طراحی شدند که تنها قادر به انجام\nوظایف محدود بودند. با گذر زمان و پیشرفت در الگوریتمها و پردازش دادهها، این سیستمها پیچیدهتر شدند. اکنون، هوش\nمصنوعی به سطحی رسیده است که میتواند بسیاری از وظایفانسانی را با دقت و سرعت انجام دهد.\nکاربردهای هوش مصنوعی بسیار گسترده است و تقریبا ً در تمامی حوزهها میتوان اثری از آن یافت. در حوزه پزشکی، هوش\nمصنوعی توانسته است به تشخیص بیماریها کمک کند. برای مثال، سیستمهایی طراحی شدهاند که میتوانند از طریق تحلیل\nتصاویر پزشکی، بیماریها را شناسایی کنند. همچنین، این فناوری در توسعه داروهای جدید نیز به کار گرفته شده است. در حوزه\nصنعت، رباتهای مجهز به هوش مصنوعی به کار گرفته شدهاند تا بهرهوری را افزایش دهند. در حوزه آموزش، سیستمهایی\nطراحی شدهاند که میتوانند به شخصیسازی آموزش کمک کنند. به طور کلی،هوش مصنوعی توانسته است به ابزاری قدرتمند\nتبدیل شود که میتواند در تمامی حوزهها تأثیرگذار باشد.\nاما این فناوری چالشهای خود را نیز دارد. یکی از چالشهای اصلی، موضوع اخالق است. بسیاری از افراد نگرانیهایی درباره\nاستفاده نادرست از هوش مصنوعی دارند. برای مثال، نگرانیهایی درباره استفاده از این فناوری برای اهداف نظامی وجود دارد.\nهمچنین، موضوع عدالت در تصمیمگیریهای مبتنی بر هوش مصنوعی نیز مطرح است. این فناوری میتواند به ایجاد تبعیض در\nجوامع منجر شود، به ویژه زمانی که الگوریتمهای آن به درستی طراحی نشده باشند. چالش دیگر، جایگزینی نیروی انسانی با\nرباتهای هوشمند است. این مسئله میتواند به بیکاری گسترده دربسیاری از حوزهها منجر شود.\nدر نهایت، آینده هوش مصنوعی یکی از مباحثی است که توجه بسیاری را به خود جلب کرده است. این فناوری در سالهای آینده\nاحتماال ً به پیشرفتهای بیشتری دست خواهد یافت و تأثیرات بیشتری بر زندگی انسان خواهد گذاشت. از پیشرفت در حوزه سالمت\nگرفته تا بهبود کیفیت زندگی، هوش مصنوعی میتواند به ابزاری قدرتمند تبدیل شود که زندگی بشر را دگرگون کند. با این حال،\nباید توجه داشت که استفاده مناسب از این فناوری نیازمند برنامهریزی دقیق و توجه به جنبههای اخالقی و اجتماعی است.\nهوش مصنوعی پدیدهای است که همچنان در حال رشد و توسعه است و میتواند در آینده نقش مهمی در شکلدهی به جهان ایفا\nکند. با وجود تمام چالشها، این فناوری میتواند به بهبود کیفیت زندگی انسان کمک کند و جهان را به مکانی بهتر تبدیل کند.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:14:34.893390Z","iopub.execute_input":"2024-12-08T12:14:34.893749Z","iopub.status.idle":"2024-12-08T12:14:34.901394Z","shell.execute_reply.started":"2024-12-08T12:14:34.893715Z","shell.execute_reply":"2024-12-08T12:14:34.900583Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n\nimport torch\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom huggingface_hub import login\n\nlogin(token = \"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\")\n# 4-bit quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,  \n    bnb_4bit_quant_type='nf4'  \n)\n\n# Model and tokenizer setup\nmodel_name = \"microsoft/Phi-3.5-mini-instruct\"\n\n# Load the model with 4-bit quantization\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    token=\"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\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", add_eos_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:14:36.594461Z","iopub.execute_input":"2024-12-08T12:14:36.594806Z","iopub.status.idle":"2024-12-08T12:18:04.683936Z","shell.execute_reply.started":"2024-12-08T12:14:36.594775Z","shell.execute_reply":"2024-12-08T12:18:04.683266Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f9e6bb9ad24ca9a3f3588328f0a0c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ce4deff3914d879497eebf23807b32"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c257fed0e940b5bd18e3b30ebf5fdd"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1207c23e8b9140eb8ca7ecc3fd419071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7f4037b9254c1ca843dba08f9d2bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9aaf3f2bf245d882ab38a150bfa2be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39db2b6ea23b42869cf8a9786f2bc154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2cf6792f4e49c988fc419f9f206ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ccfb162632490eb7dc79461bedc798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca96a7211dd94c0781f6a29ea6d345c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c6702cd7834f47ac63139b300171a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8829fccfd734a2bb1ac023f0391a6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0783002a36b44c9a98a16e4660a6c82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bcd42fb6eeb48a98d71fa1683c37201"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\nlora_config = LoraConfig(\n    r=16,  \n    lora_alpha=32, \n    target_modules=[\n        \"self_attn.q_proj\",  \n        \"self_attn.k_proj\",  \n        \"self_attn.v_proj\", \n        \"self_attn.o_proj\",  \n        \"mlp.up_proj\",     \n        \"mlp.down_proj\"      \n    ],\n    lora_dropout=0.1, \n    task_type=\"CAUSAL_LM\"  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:04.685291Z","iopub.execute_input":"2024-12-08T12:18:04.685764Z","iopub.status.idle":"2024-12-08T12:18:04.690106Z","shell.execute_reply.started":"2024-12-08T12:18:04.685737Z","shell.execute_reply":"2024-12-08T12:18:04.689195Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# Prepare the model for fine-tuning \nbase_model = prepare_model_for_kbit_training(base_model)\n\n# Enable gradient checkpointing to save memory\nbase_model.gradient_checkpointing_enable()\n\n# Apply LoRA to the base model\npeft_model = get_peft_model(base_model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:04.690970Z","iopub.execute_input":"2024-12-08T12:18:04.691221Z","iopub.status.idle":"2024-12-08T12:18:04.900481Z","shell.execute_reply.started":"2024-12-08T12:18:04.691198Z","shell.execute_reply":"2024-12-08T12:18:04.899549Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\nfrom datasets import Dataset\n\n# Create a Hugging Face dataset from the full dataset\ntrain_dataset = Dataset.from_dict({\"chunks\": raw_text})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:04.902668Z","iopub.execute_input":"2024-12-08T12:18:04.903139Z","iopub.status.idle":"2024-12-08T12:18:05.306378Z","shell.execute_reply.started":"2024-12-08T12:18:04.903097Z","shell.execute_reply":"2024-12-08T12:18:05.305671Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"chunks\"], padding=\"max_length\", truncation=True, max_length=2048)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:05.307496Z","iopub.execute_input":"2024-12-08T12:18:05.307848Z","iopub.status.idle":"2024-12-08T12:18:05.311838Z","shell.execute_reply.started":"2024-12-08T12:18:05.307805Z","shell.execute_reply":"2024-12-08T12:18:05.310969Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:05.312767Z","iopub.execute_input":"2024-12-08T12:18:05.312968Z","iopub.status.idle":"2024-12-08T12:18:08.337573Z","shell.execute_reply.started":"2024-12-08T12:18:05.312947Z","shell.execute_reply":"2024-12-08T12:18:08.336826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0339a2f4dbdf4ba9ad731d94300730d1"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"\n\n# from sklearn.model_selection import train_test_split\n\n# tokenized_train_dataset, val_dataset = train_test_split(tokenized_train_dataset, test_size = 0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:08.338769Z","iopub.execute_input":"2024-12-08T12:18:08.339158Z","iopub.status.idle":"2024-12-08T12:18:08.343215Z","shell.execute_reply.started":"2024-12-08T12:18:08.339118Z","shell.execute_reply":"2024-12-08T12:18:08.342407Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(f\"Original dataset columns: {train_dataset.column_names}\")\nprint(f\"Tokenized dataset columns: {tokenized_train_dataset.column_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:18:08.344174Z","iopub.execute_input":"2024-12-08T12:18:08.344429Z","iopub.status.idle":"2024-12-08T12:18:08.357779Z","shell.execute_reply.started":"2024-12-08T12:18:08.344405Z","shell.execute_reply":"2024-12-08T12:18:08.357084Z"}},"outputs":[{"name":"stdout","text":"Original dataset columns: ['chunks']\nTokenized dataset columns: ['chunks', 'input_ids', 'attention_mask']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:21:13.757888Z","iopub.execute_input":"2024-12-08T12:21:13.758744Z","iopub.status.idle":"2024-12-08T12:21:13.764350Z","shell.execute_reply.started":"2024-12-08T12:21:13.758692Z","shell.execute_reply":"2024-12-08T12:21:13.762909Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\n\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./ft-phi3.5-model\",\n    do_train=True,\n    # do_eval=True,\n    # logging_strategy=\"steps\",\n    # eval_strategy=\"steps\",  # تغییر به \"steps\"\n    per_device_train_batch_size=1,\n    num_train_epochs=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=2,\n    learning_rate=1e-4,\n    # evaluation_strategy=\"steps\",  # تغییر به \"steps\"\n    logging_steps=20,\n    save_steps=20,\n    save_total_limit=5,\n    fp16=True,\n    # save_strategy=\"steps\",  # تغییر به \"steps\"\n    report_to=\"wandb\",\n    logging_dir=\"./logs\",\n    overwrite_output_dir=True,\n    # load_best_model_at_end=True,\n    # metric_for_best_model=\"eval_loss\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:20:49.726136Z","iopub.execute_input":"2024-12-08T12:20:49.726523Z","iopub.status.idle":"2024-12-08T12:20:51.225737Z","shell.execute_reply.started":"2024-12-08T12:20:49.726494Z","shell.execute_reply":"2024-12-08T12:20:51.224793Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n\nfrom transformers import EarlyStoppingCallback\nfrom torch.optim import AdamW\n\noptimizer = AdamW(base_model.parameters(), lr=1e-4, weight_decay= 0.1)\nearly_stop = EarlyStoppingCallback(early_stopping_patience=20)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    # eval_dataset= val_dataset,\n    data_collator=data_collator,\n    #callbacks=[early_stop],\n    tokenizer=tokenizer,\n    #optimizers= (optimizer, None)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:21:19.250588Z","iopub.execute_input":"2024-12-08T12:21:19.250919Z","iopub.status.idle":"2024-12-08T12:21:20.252528Z","shell.execute_reply.started":"2024-12-08T12:21:19.250888Z","shell.execute_reply":"2024-12-08T12:21:20.251622Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3949065322.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:21:20.898607Z","iopub.execute_input":"2024-12-08T12:21:20.899076Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241208_122234-54o1ykil</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/54o1ykil' target=\"_blank\">./ft-phi3.5-model</a></strong> to <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface' target=\"_blank\">https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/54o1ykil' target=\"_blank\">https://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/54o1ykil</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='826' max='3724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 826/3724 5:35:26 < 19:39:44, 0.04 it/s, Epoch 0.89/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>6.398300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.032200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.926600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.013000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.938300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.739000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.853600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.724000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.740500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.739500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.831000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.929600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.710300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.663100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.787900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.647100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.804300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.712000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.757100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.619300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.835100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.774500</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.724900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.747400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.604900</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.762300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.792000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.789100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.710000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.641100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.776000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.720100</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.664600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.622600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.644900</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.683100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.741100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.737300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.733300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.766700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.753000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n\nmodell = AutoModelForCausalLM.from_pretrained(\n    \"./ft-phi3.5-model\", \n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\n\n\neval_tokenizer = AutoTokenizer.from_pretrained(\"./ft-phi3.5-model\", add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\n\nfrom peft import PeftModel \n\nft_model = PeftModel.from_pretrained(\n    modell, \n    \"path-to-checkpoint-860\", \n    torch_dtype=torch.float16,\n    is_trainable=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:22.628783Z","iopub.status.idle":"2024-12-08T12:13:22.629096Z","shell.execute_reply.started":"2024-12-08T12:13:22.628929Z","shell.execute_reply":"2024-12-08T12:13:22.628945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n# Initialize text generation pipelines for both models\npretrained_pipeline = pipeline('text-generation', model=model, tokenizer=eval_tokenizer)\nposttrained_pipeline = pipeline('text-generation', model=ft_model, tokenizer=eval_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:22.630135Z","iopub.status.idle":"2024-12-08T12:13:22.630422Z","shell.execute_reply.started":"2024-12-08T12:13:22.630284Z","shell.execute_reply":"2024-12-08T12:13:22.630300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef generate_response(pipeline, query):\n    response = pipeline(query, max_length=300, do_sample=False)\n    return response[0]['generated_text']\n\ndef compare_responses(pre_fine_tuned_response, post_fine_tuned_response):\n    print(f\"{'=' * 50}\\nResponse Comparison\\n{'=' * 50}\")\n    print(f\"{'Pre-Fine-tuned Response:'}\\n{pre_fine_tuned_response}\\n\")\n    print(f\"{'Post-Fine-tuned Response:'}\\n{post_fine_tuned_response}\\n\")\n\n    pre_length = len(pre_fine_tuned_response.split())\n    post_length = len(post_fine_tuned_response.split())\n    print(f\"{'=' * 50}\")\n    print(f\"Word Count:\\nPre-Fine-tuned: {pre_length} words\\nPost-Fine-tuned: {post_length} words\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:22.631530Z","iopub.status.idle":"2024-12-08T12:13:22.631816Z","shell.execute_reply.started":"2024-12-08T12:13:22.631677Z","shell.execute_reply":"2024-12-08T12:13:22.631693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nquery = \"\"\"در بحث تکنیک های هوش مصنوعی در رابطه با پردازش زبان طبیعی توضیح بده .\"\"\"\n\npre_finetune_response = generate_response(pretrained_pipeline, query)\npost_finetune_response = generate_response(posttrained_pipeline, query)\n\ncompare_responses(pre_finetune_response, post_finetune_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:22.633202Z","iopub.status.idle":"2024-12-08T12:13:22.633467Z","shell.execute_reply.started":"2024-12-08T12:13:22.633337Z","shell.execute_reply":"2024-12-08T12:13:22.633351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport re\nimport pdfplumber\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom huggingface_hub import login\n\nlogin(token = \"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)\n\nmodel_name = \"microsoft/Phi-3.5-mini-instruct\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, is_decoder = True).to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef read_pdf(pdf_path):\n    text = \"\"\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text() + \"\\n\"\n\n    return text\n\ndef clean_text(text):\n    text = re.sub(\"\\s+\", \" \", text)\n    return text.strip()\n\npdf_path = \"path_for_pdf\"\ndata = clean_text(read_pdf(pdf_path))\n\nchunk_size = 512\nchunks = [data[i: i+chunk_size] for i in range(0, len(data), chunk_size)]\n\ntrain_data, val_data = train_test_split(chunks, test_size = 0.1, random_state = 42)\n\ntrain_inputs = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = 'max_length'\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\",abs\n)\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()\n\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inupts['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\" : input_ids[idx],\n            \"attention_mask\" : attention_mask[idx],\n            \"labels\" : labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_inputs)\neval_dataset = TextDataset(val_inputs)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        \"self_attn.q_proj\",\n        \"self_attn.k_proj\",\n        \"self_attn.v_proj\",\n        \"self_attn.o_proj\",\n        \"mlp.up_proj\",\n        \"mlp.down_proj\"\n    ],\n    lora_dropout=0.1,\n    task_type=\"CAUSAL_LM\"\n)\n\n# آماده‌سازی مدل برای آموزش با کم‌حجم‌سازی\nmodel = prepare_model_for_kbit_training(model, lora_config)\n\ntraining_arguments = TrainingArguments(\n    output_dir = \"./Result\",\n    overwrite_output_dir= True,\n    do_train = True,\n    do_evla = True,\n    save_strategy = \"steps\",\n    eval_strategy = \"steps\",\n    logging_steps = \"steps\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    lr_scheduler_type = \"cosine\",\n    warmup_ratio = 0.1,\n    logging_steps = 10,\n    learning_rate= 2e-5,\n    save_steps= 500,\n    num_train_epochs= 15,\n    max_steps= 1500,\n    max_grad_norm= 1.0,\n    quantization_ratio = 16.0,\n    loraplus_bit = 8,\n    loraplus_lr_ratio = 0.5,\n    fp16= True,\n    load_best_model_at_end= True,\n    metric_for_best_model= \"eval_loss\",\n)\n\n\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\nfrom transformers import EarlyStoppingCallback\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr = 2e-5, weight_decay = 0.1)\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    args = training_arguments,\n    train_dataset= train_dataset,\n    eval_dataset = eval_dataset,\n    callbacks = [early_stop_callback],\n    optimizers = (optimizer, None)\n)\n\ntrainer.train()\n\nmodel.save_pretrained(\"./fine_tuning_model_phi\")\ntokenizer.save_pretrained(\"./fine_tuning_model_phi\")\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:13:22.635449Z","iopub.status.idle":"2024-12-08T12:13:22.635748Z","shell.execute_reply.started":"2024-12-08T12:13:22.635610Z","shell.execute_reply":"2024-12-08T12:13:22.635625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport re\nimport pdfplumber\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLm\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import EarlyStoppingCallback\nfrom sklearn.model_selection import train_test_split\nfrom huggingface_hub import login\n\nlogin(token = \"hugging_face_token\")\n\nmodel_name = \"model_name\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, is_decoder = True).to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef read_pdf(pdf_path):\n    text = \"\"\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text() + \"\\n\"\n\n    return text\n\ndef clean_data(text):\n    text = re.sub(\"\\s+\", \" \", text)\n    return text.strip()\n\n\npdf_path = \"path_for_pdf_test\"\ndata = clean_data(read_pdf(pdf_path))\n\nchunk_size = 512\nchunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n\ntrain_data, val_data = train_test_split(chunks, test_size = 0.1, random_state = 42)\n\ntrain_input = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\"\n)\n\neval_input = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\"\n)\n\ntrain_input['labels'] = train_input['input_ids'].clone()\neval_input['labels'] = eval_input['input_ids'].clone()\n\nclass TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem(self, idx):\n        return {\n            \"input_ids\" : self.input_ids[idx],\n            \"attention_mask\" : self.attention_mask[idx],\n            \"labels\" : self.labels[idx]\n        }\n\ntrain_dataset = TextDataset(train_input)\neval_dataset = TextDataset(eval_input)\n\ntraining_arguments = TrainingArguments(\n    output_dir = \"./Result\",\n    overwrite_output_dir = True,\n    do_train = True,\n    do_eval = True,\n    save_strategy = \"steps\",\n    logging_strategy = \"steps\",\n    eval_strategy = \"steps\",\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 4,\n    lr_scheduler_type = \"cosine\",\n    logging_steps = 10,\n    warmup_ratio = 0.1,\n    learning_rate = 5e-5,\n    save_steps = 200,\n    num_train_epochs = 15,\n    max_steps = 1500,\n    max_grad_norm = 1.0,\n    quantization_bit = 4,\n    report_to = \"wandb\",\n    load_best_model_at_end = True,\n    metric_for_best_model = \"eval_loss\"\n)\n\nfrom transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stop_patience = 2)\noptimizer = AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.1)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    train_dataset= train_dataset,\n    eval_dataset = eval_dataset,\n    args = training_arguments,\n    callbacks = [early_stop_callback],\n    optimizers = (optimizer, None)\n)\n\ntrainer.train()\n\nmodel.save_pretrained(\"./fine_tuning_model\")\ntokenizer.save_pretrained(\"./fine_tuning_model\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}