{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QdCCLggwz8D",
        "outputId": "133908fb-3613-4f28-927b-c73b558da178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install torchvision\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCjIsKRyxlmj",
        "outputId": "067cd433-5a96-44cd-8e71-fbcc37e0c760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90gVSJzNxHis"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"CohereForAI/aya-23-8B\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "import pdfplumber\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages():\n",
        "      text += page.extract_text() + \"\\n\"\n",
        "\n",
        "  return text\n",
        "\n",
        "pdf_path = \"path_for_pdf\"\n",
        "data = read_pdf(pdf_path)\n",
        "\n",
        "inputs = tokenizer(data, return_tensors = 'pt', max_length = 256, trunction = True, padding = 'max_length')\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = './Result',\n",
        "    do_train = True,\n",
        "    overwrite_output_dir= True,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    lr_scheduler_type = 'cosine',\n",
        "    logging_step = 10,\n",
        "    warmup_ratio= 0.1\n",
        "    learning_rate= 5e-5,\n",
        "    save_steps = 1000,\n",
        "    num_train_epochs= 3,\n",
        "    max_steps = 500,\n",
        "    max_grad_norm = 1.0,\n",
        "    quanization_bit = 4,\n",
        "    loraplus_lr_ratio = 16.0,\n",
        "    save_total_limit= 2,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = inputs['input_ids']\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pratrained('./fine_tuning_model')\n",
        "tokenizer.save_pretrained('./finetuning_moddel')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1JEKXWcP0_RT",
        "outputId": "7c817a36-1ed7-4a8e-de90-e9903a69884b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-afa2ed34797e>:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 00:44, Epoch 500/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.707900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.638800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.529600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.256200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.079600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.047200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.033100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.013100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.011300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.004500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.033400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.json',\n",
              " './fine_tuned_model/merges.txt',\n",
              " './fine_tuned_model/added_tokens.json',\n",
              " './fine_tuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import pdfplumber\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = read_pdf(pdf_path)\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    data,\n",
        "    return_tensors='pt',\n",
        "    max_length=256,\n",
        "    truncation=True,\n",
        "    padding='max_length'\n",
        ")\n",
        "\n",
        "inputs['labels'] = inputs['input_ids'].clone()\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.input_ids = inputs[\"input_ids\"]\n",
        "        self.attention_mask = inputs[\"attention_mask\"]\n",
        "        self.labels = inputs[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(inputs)\n",
        "\n",
        "# ایجاد Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# فاین‌تیون مدل\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "model.save_pretrained('./fine_tuned_model')\n",
        "tokenizer.save_pretrained('./fine_tuned_model')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEl6I6bd1BIf",
        "outputId": "23694f43-2af9-4716-df4c-f0152febf1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\n",
            "Answer: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بدهت‌همانرب\n",
            "یسایق‌هیضار\n",
            "Raziehghiasi@gmail.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_path = \"./fine_tuned_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "question = \"در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\"\n",
        "answer = ask_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "avo7r35G2lRX",
        "outputId": "3738e77e-9942-4e0c-859a-ce9ed56dfb7c"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "HooshvareLab/gpt2-medium-fa is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/HooshvareLab/gpt2-medium-fa/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6749b263-1996d21730ceaffc6ba7c6ad;805d15e4-2738-452a-9c62-0a9bfa347458)\n\nRepository Not Found for url: https://huggingface.co/HooshvareLab/gpt2-medium-fa/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b554d457317>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# بارگذاری مدل و توکنایزر\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: HooshvareLab/gpt2-medium-fa is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "import pdfplumber\n",
        "\n",
        "login(token = \"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\")\n",
        "\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-medium-fa\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استفاده از eos به عنوان پدینگ\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = read_pdf(pdf_path)\n",
        "\n",
        "# آماده‌سازی داده‌ها با اضافه کردن labels\n",
        "inputs = tokenizer(\n",
        "    data,\n",
        "    return_tensors='pt',\n",
        "    max_length=256,\n",
        "    truncation=True,\n",
        "    padding='max_length'\n",
        ")\n",
        "\n",
        "inputs['labels'] = inputs['input_ids'].clone()\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.input_ids = inputs[\"input_ids\"]\n",
        "        self.attention_mask = inputs[\"attention_mask\"]\n",
        "        self.labels = inputs[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(inputs)\n",
        "\n",
        "# تنظیمات فاین‌تیونینگ\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir='./Result',\n",
        "    do_train=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=10,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=5e-5,\n",
        "    save_steps=1000,\n",
        "    num_train_epochs=3,\n",
        "    max_steps=500,\n",
        "    max_grad_norm=1.0,\n",
        "    save_total_limit=2,\n",
        "    report_to=None  # غیرفعال کردن W&B\n",
        ")\n",
        "\n",
        "# ایجاد Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# فاین‌تیون مدل\n",
        "trainer.train()\n",
        "\n",
        "# ذخیره مدل و توکنایزر\n",
        "model.save_pretrained('./fine_tuned_model')\n",
        "tokenizer.save_pretrained('./fine_tuned_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g10tQZehmci6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kEUfxy2l3Y7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b968eac6-17a6-4f3c-e2b6-5cf7ee66ffa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio\n",
        "!pip install torchvision\n",
        "!pip install transformers\n",
        "!pip install diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZxpo-GTlxE4",
        "outputId": "6c6dff7d-9730-4e2b-c7f1-16827a5983de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (11.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBESJcBBmwiy",
        "outputId": "b2c4781b-1459-41b1-f1b2-69dcc607efcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pdfplumber\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder = True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "      text += page.extract_text() + \"\\n\"\n",
        "\n",
        "  return text\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(\"\\s+\", \" \", text)\n",
        "  text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n",
        "  return text.strip()\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = clean_text(read_pdf(pdf_path))\n",
        "\n",
        "chunk_size = 256\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "train_inputs, val_inputs = train_test_split(chunks, test_size = 0.1, random_state = 42)\n",
        "\n",
        "train_dataset = tokenizer(\n",
        "    train_inputs,\n",
        "    return_tensors = 'pt',\n",
        "    max_length = chunk_size,\n",
        "    truncation = True,\n",
        "    padding = 'max_length',\n",
        ")\n",
        "\n",
        "val_dataset = tokenizer(\n",
        "    val_inputs,\n",
        "    return_tensors = 'pt',\n",
        "    max_length = chunk_size,\n",
        "    truncation = True,\n",
        "    padding = 'max_length',\n",
        ")\n",
        "\n",
        "train_inputs['labels'] = train_inputs['input_ids'].clone()\n",
        "val_inputs['labels'] = val_inputs['input_ids'].clone()\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, inputs):\n",
        "    self.input_ids = inputs['input_ids']\n",
        "    self.attention_mask = inputs['attention_mask']\n",
        "    self.labels = inputs['labels']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids):\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        \"input_ids\" : self.input_ids[idx],\n",
        "        \"attention_mask\" : self.attention_mask[idx],\n",
        "        \"labels\" : self.labels[idx],\n",
        "    }\n",
        "\n",
        "\n",
        "train_dataset = TextDataset(train_dataset)\n",
        "val_dataset = TextDataset(val_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "bDo7PxCa549d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pdfplumber\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = clean_text(read_pdf(pdf_path))\n",
        "\n",
        "\n",
        "chunk_size = 256\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "train_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "train_inputs = tokenizer(\n",
        "    train_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "val_inputs = tokenizer(\n",
        "    val_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "train_inputs['labels'] = train_inputs['input_ids'].clone()\n",
        "val_inputs['labels'] = val_inputs['input_ids'].clone()\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.input_ids = inputs[\"input_ids\"]\n",
        "        self.attention_mask = inputs[\"attention_mask\"]\n",
        "        self.labels = inputs[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(train_inputs)\n",
        "val_dataset = TextDataset(val_inputs)\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = \"./result\",\n",
        "    overwrite_output_dir = True,\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    save_strategy = 'steps',\n",
        "    eval_strategy = 'steps',\n",
        "    logging_strategy = 'steps',\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    lr_scheduler_type = 'cosine',\n",
        "    logging_steps = 10,\n",
        "    warmup_ratio = 0.1,\n",
        "    learning_rate = 2e-5,\n",
        "    num_train_epochs = 10,\n",
        "    save_steps = 500,\n",
        "    max_steps = 1500,\n",
        "    max_grad_norm = 1.0,\n",
        "    save_total_limit = 2,\n",
        "    eval_steps = 100,\n",
        "    report_to = None,\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_arguments,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./fine_tuned_model_2\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model_2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "DTmTNYcOtjSu",
        "outputId": "701efae9-6816-46d1-8b73-f1dd91af32f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 16:04, Epoch 133/137]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.027800</td>\n",
              "      <td>4.660548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.899800</td>\n",
              "      <td>3.364281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.799500</td>\n",
              "      <td>3.122620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.171900</td>\n",
              "      <td>3.104553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.724600</td>\n",
              "      <td>3.144642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.481300</td>\n",
              "      <td>3.181426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.349600</td>\n",
              "      <td>3.236079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.252300</td>\n",
              "      <td>3.298655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.204000</td>\n",
              "      <td>3.335447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>3.355368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.156100</td>\n",
              "      <td>3.382314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.153200</td>\n",
              "      <td>3.397170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.138800</td>\n",
              "      <td>3.401043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>3.405062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.136700</td>\n",
              "      <td>3.404515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_model_2/tokenizer_config.json',\n",
              " './fine_tuned_model_2/special_tokens_map.json',\n",
              " './fine_tuned_model_2/vocab.json',\n",
              " './fine_tuned_model_2/merges.txt',\n",
              " './fine_tuned_model_2/added_tokens.json',\n",
              " './fine_tuned_model_2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pdfplumber\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "\n",
        "model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = clean_text(read_pdf(pdf_path))\n",
        "\n",
        "\n",
        "chunk_size = 256\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "train_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "train_inputs = tokenizer(\n",
        "    train_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "val_inputs = tokenizer(\n",
        "    val_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "train_inputs[\"labels\"] = train_inputs[\"input_ids\"].clone()\n",
        "val_inputs[\"labels\"] = val_inputs[\"input_ids\"].clone()\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.input_ids = inputs[\"input_ids\"]\n",
        "        self.attention_mask = inputs[\"attention_mask\"]\n",
        "        self.labels = inputs[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(train_inputs)\n",
        "val_dataset = TextDataset(val_inputs)\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./Result\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    save_steps=500,\n",
        "    max_steps = 2000,\n",
        "    num_train_epochs=10,\n",
        "    max_grad_norm=1.0,\n",
        "    save_total_limit=2,\n",
        "    eval_steps=100,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "e5rmGO9ul8C8",
        "outputId": "292f3d96-6192-4000-9400-58d00e6f6c0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 28:04, Epoch 177/182]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.501800</td>\n",
              "      <td>2.958564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.146600</td>\n",
              "      <td>0.169391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.067638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.038458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.047055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.026494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.046187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.028673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.025057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.043567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.042200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.050346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.034093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.035204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.030764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.029966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.029351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.029858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.029390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.029385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.txt',\n",
              " './fine_tuned_model/added_tokens.json',\n",
              " './fine_tuned_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_path = \"./fine_tuned_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, do_decode = True)\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "question = \"در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\"\n",
        "answer = ask_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJkULX3md8L",
        "outputId": "f95ddacb-e861-4e66-94f4-def427545ac2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\n",
            "Answer: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده..................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_path = \"./fine_tuned_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "question = \"در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\"\n",
        "answer = ask_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W5p9PK9pU_4",
        "outputId": "e69afdc7-d595-47be-9aa6-42a8491474a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده\n",
            "Answer: در رابطه با بحث شی گرایی د ر سی پلاس پلاس توضیح بده..................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_path = \"./fine_tuned_model_2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "question = \"سلام\"\n",
        "answer = ask_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nHpNjUpzPQa",
        "outputId": "04a91bce-8f28-4a88-c480-398cf1ffd490"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: سلام\n",
            "Answer: سلام  ییارگی ش 54 هتکن یور یصاخ یزاسهنیهب تامادقا رلیاپماک و دوش ییاراک شیازفا ثعاب دناوتیم تباث تروص هب ایشا و اهریغتم نلاعا  دهدیم ماجنا اهتباث بوسحم یرمارگ یاطخ دهدیم رییغت ار یش کی یاهداد یاضعا هک یلاحرد تباث تروص هب وضع عب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pdfplumber\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import AdamW\n",
        "import re\n",
        "\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = clean_text(read_pdf(pdf_path))\n",
        "\n",
        "\n",
        "chunk_size = 256\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(chunks, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "train_inputs = tokenizer(\n",
        "    train_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "val_inputs = tokenizer(\n",
        "    val_data,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=chunk_size,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "train_inputs['labels'] = train_inputs['input_ids'].clone()\n",
        "val_inputs['labels'] = val_inputs['input_ids'].clone()\n",
        "\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.input_ids = inputs[\"input_ids\"]\n",
        "        self.attention_mask = inputs[\"attention_mask\"]\n",
        "        self.labels = inputs[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "\n",
        "train_dataset = TextDataset(train_inputs)\n",
        "val_dataset = TextDataset(val_inputs)\n",
        "\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./result\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=20,\n",
        "    save_steps=500,\n",
        "    max_steps=1500,\n",
        "    max_grad_norm=1.0,\n",
        "    save_total_limit=2,\n",
        "    eval_steps=100,\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "# تنظیمات DataCollator برای آموزش\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # حالت ماسک کردن کلمات برای مدل زبان\n",
        ")\n",
        "\n",
        "# استفاده از EarlyStoppingCallback برای جلوگیری از overfitting\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "# استفاده از AdamW به جای Adam\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[early_stopping_callback],\n",
        "    optimizers=(optimizer, None),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "model.save_pretrained(\"./fine_tuned_model_3\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model_3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "P9MAsXKZ9niI",
        "outputId": "c81c14c0-53b0-457b-99dd-a37aa0c8c89b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 800/1500 13:52 < 12:10, 0.96 it/s, Epoch 145/300]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.915500</td>\n",
              "      <td>4.450997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.484700</td>\n",
              "      <td>3.232180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.318800</td>\n",
              "      <td>3.098987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>3.140909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.374800</td>\n",
              "      <td>3.256903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>3.344218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.144800</td>\n",
              "      <td>3.444175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.098500</td>\n",
              "      <td>3.521127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_model_3/tokenizer_config.json',\n",
              " './fine_tuned_model_3/special_tokens_map.json',\n",
              " './fine_tuned_model_3/vocab.json',\n",
              " './fine_tuned_model_3/merges.txt',\n",
              " './fine_tuned_model_3/added_tokens.json',\n",
              " './fine_tuned_model_3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pdfplumber\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from huggingface_hub import login\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, is_decoder = True).to(device)\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "      text += page.extract_text() + \"\\n\"\n",
        "\n",
        "  return text\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(\"\\s+\", \" \", text)\n",
        "  text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n",
        "  return text.strip()\n",
        "\n",
        "pdf_path = \"/content/Chapter 2_Object Oriented.pdf\"\n",
        "data = clean_text(read_pdf(pdf_path))\n",
        "\n",
        "chunk_size = 256\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "train_data, val_data = train_test_split(chunks, test_size = 0.1, random_state = 42)\n",
        "\n",
        "train_inputs = tokenizer(\n",
        "    train_data,\n",
        "    return_tensors = 'pt',\n",
        "    max_length = chunk_size,\n",
        "    truncation = True,\n",
        "    padding = 'max_length',\n",
        ")\n",
        "\n",
        "val_inputs = tokenizer(\n",
        "    val_data,\n",
        "    return_tensors = 'pt',\n",
        "    max_length = chunk_size,\n",
        "    truncation = True,\n",
        "    padding = 'max_length'\n",
        ")\n",
        "\n",
        "train_inputs['labels'] = train_inputs['labels'].clone()\n",
        "val_inputs['labels'] = val_inputs['labels']"
      ],
      "metadata": {
        "id": "ZMsVfjhs-zJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_path = \"./fine_tuned_model_2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "question = \"سلام\"\n",
        "answer = ask_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql4K6D2E-Rgf",
        "outputId": "de10662f-2f01-4fd1-9509-b52bb4294c62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: سلام\n",
            "Answer: سلام ییارگی ش 54 هتکن یور یصاخ یزاسهنیهب تامادقا رلیاپماک و دوش ییاراک شیازفا ثعاب دناوتیم تباث تروص هب ایشا و اهریغتم نلاعا  دهدیم ماجنا اهتباث بوسحم یرمارگ یاطخ دهدیم رییغت ار یش کی یاهداد یاضعا هک یلاحرد تباث تروص هب وضع عبات\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "citY70kfDJ11"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}