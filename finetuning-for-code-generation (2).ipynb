{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport re\nimport os\nimport transformers \nfrom torch.utils.data import Dataset\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom huggingface_hub import login\n\n\nmodel_name = \"gpt2-large\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, is_decoder= True).to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:04:08.994089Z","iopub.execute_input":"2024-12-15T13:04:08.994397Z","iopub.status.idle":"2024-12-15T13:04:52.232196Z","shell.execute_reply.started":"2024-12-15T13:04:08.994364Z","shell.execute_reply":"2024-12-15T13:04:52.231217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"709d028b8b174466ad2b2edfefebe8e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1946613ce9f41a7a5c575d72f6df641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93823d9611654853a7f5cf3e69b3bddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2259e68ef6402797d534c3c8af222c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7228a4075fb141b39936eefe33e35b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e633a312ccc44724993959d3c8bdc724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a15c8e7cd445dca83eae49685c892a"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n\ninput_prompts = [\n    \"Write a function to calculate the factorial of a number.\",\n    \"How can you reverse a string in Python?\",\n    \"Create a function to check if a number is a palindrome.\",\n    \"Write a program to find the greatest common divisor (GCD) of two numbers.\",\n    \"How can you implement the Fibonacci sequence in Python?\",\n    \"Write a Python script to read a CSV file and print its contents.\",\n    \"How do you create a new text file and write data into it in Python?\",\n    \"Write a program to count the number of lines in a text file.\",\n    \"How can you append data to an existing file in Python?\",\n    \"Write a script to search for a specific word in a file.\",\n    \"Implement a stack using a Python list.\",\n    \"How do you implement a queue using a Python list?\",\n    \"Write a program to sort a list of integers in ascending order.\",\n    \"How can you merge two sorted lists in Python?\",\n    \"Create a function to find the second largest number in a list.\",\n    \"Write a program to check if two strings are anagrams.\",\n    \"How do you count the occurrences of each element in a list?\",\n    \"Write a Python function to flatten a nested list.\",\n    \"How can you generate all permutations of a string in Python?\",\n    \"Implement a binary search algorithm for a sorted list.\",\n    \"Write a function to find the depth of a binary tree.\",\n    \"How can you implement a linked list in Python?\",\n    \"Write a program to traverse a binary tree in in-order, pre-order, and post-order.\",\n    \"How do you find the shortest path in a graph using Dijkstra's algorithm?\",\n    \"Create a function to validate if a given string has balanced parentheses.\",\n    \"Write a program to find the longest substring without repeating characters.\",\n    \"How can you remove duplicates from a list in Python?\",\n    \"Write a function to calculate the power of a number using recursion.\",\n    \"How do you implement a simple calculator in Python?\",\n    \"Create a function to find all prime numbers up to a given limit.\"\n]\n\nlarge_dataset = []\nfor prompt in input_prompts:\n    generated_texts = generator(prompt, max_length=1024, num_return_sequences=15, temperature=0.7)\n    for gen in generated_texts:\n        large_dataset.append(gen['generated_text'])\n\n\nwith open(\"generated_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    for text in large_dataset:\n        f.write(text + \"\\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:04:56.153998Z","iopub.execute_input":"2024-12-15T13:04:56.154891Z","iopub.status.idle":"2024-12-15T13:48:03.703614Z","shell.execute_reply.started":"2024-12-15T13:04:56.154842Z","shell.execute_reply":"2024-12-15T13:48:03.702905Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nchunk_size = 512\nchunks = [text[i:i+chunk_size] for text in large_dataset for i in range(0, len(text), chunk_size)]\n\ntrain_data, val_data = train_test_split(chunks, test_size = 0.1, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:59:15.425586Z","iopub.execute_input":"2024-12-15T13:59:15.426283Z","iopub.status.idle":"2024-12-15T13:59:15.445768Z","shell.execute_reply.started":"2024-12-15T13:59:15.426251Z","shell.execute_reply":"2024-12-15T13:59:15.444962Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_inputs = tokenizer(\n    train_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\",\n)\n\nval_inputs = tokenizer(\n    val_data,\n    return_tensors = 'pt',\n    max_length = chunk_size,\n    truncation = True,\n    padding = \"max_length\",\n)\n\n\ntrain_inputs['labels'] = train_inputs['input_ids'].clone()\nval_inputs['labels'] = val_inputs['input_ids'].clone()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:59:50.115912Z","iopub.execute_input":"2024-12-15T13:59:50.116895Z","iopub.status.idle":"2024-12-15T13:59:51.359615Z","shell.execute_reply.started":"2024-12-15T13:59:50.116824Z","shell.execute_reply":"2024-12-15T13:59:51.358657Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\" : self.input_ids[idx],\n            \"attention_mask\" : self.attention_mask[idx],\n            \"labels\" : self.labels[idx]\n        }\n\n\ntrain_dataset = TextDataset(train_inputs)\neval_dataset = TextDataset(val_inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:59:51.486140Z","iopub.execute_input":"2024-12-15T13:59:51.487140Z","iopub.status.idle":"2024-12-15T13:59:51.493435Z","shell.execute_reply.started":"2024-12-15T13:59:51.487096Z","shell.execute_reply":"2024-12-15T13:59:51.492461Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"new_model_name = 'gpt2'\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(new_model_name)\nmodel = AutoModelForCausalLM.from_pretrained(new_model_name).to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:08:08.519576Z","iopub.execute_input":"2024-12-15T14:08:08.519947Z","iopub.status.idle":"2024-12-15T14:08:12.561815Z","shell.execute_reply.started":"2024-12-15T14:08:08.519918Z","shell.execute_reply":"2024-12-15T14:08:12.560908Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68dd2d24e0b47f29e86e25cd4d4d8fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7fb3506e724ab7a6e20d73c12ce8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f8f77e4c044b1c8d8cd2497b4880cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b97552e86ba4b0c9e32f9e1b19e0ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74571f74b48a438bb8a75b9227dfcf15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b01df55389544d29feddf2fb1f4bfdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd5e4126e8c4ad88c464308cd17a7eb"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir = \"./result\",\n    overwrite_output_dir= True,\n    do_train = True,\n    do_eval = True,\n    save_strategy = \"steps\",\n    eval_strategy = \"steps\",\n    logging_strategy = \"steps\",\n    per_device_train_batch_size = 4,\n    gradient_accumulation_steps = 2,\n    lr_scheduler_type = \"cosine\",\n    logging_steps = 10,\n    warmup_ratio = 0.1,\n    save_steps = 100,\n    learning_rate = 1e-5,\n    num_train_epochs = 8,\n    max_steps = 500,\n    max_grad_norm = 1.0,\n    report_to = \"none\",\n    load_best_model_at_end = True,\n    metric_for_best_model = \"eval_loss\",\n    fp16 = True\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:09:15.907248Z","iopub.execute_input":"2024-12-15T14:09:15.907658Z","iopub.status.idle":"2024-12-15T14:09:15.942639Z","shell.execute_reply.started":"2024-12-15T14:09:15.907618Z","shell.execute_reply":"2024-12-15T14:09:15.941982Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.optim import AdamW\n\nearly_stop_callback = EarlyStoppingCallback(early_stopping_patience = 2)\noptimizer = AdamW(model.parameters(), lr = 1e-5, weight_decay = 0.1)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,\n    mlm = False\n)\n\ntrainer = Trainer(\n    model = model,\n    data_collator = data_collator,\n    args = training_arguments,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    optimizers = (optimizer, None),\n    callbacks = [early_stop_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:09:17.058664Z","iopub.execute_input":"2024-12-15T14:09:17.059361Z","iopub.status.idle":"2024-12-15T14:09:17.073954Z","shell.execute_reply.started":"2024-12-15T14:09:17.059326Z","shell.execute_reply":"2024-12-15T14:09:17.073106Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:44:19.742509Z","iopub.execute_input":"2024-12-15T14:44:19.743300Z","iopub.status.idle":"2024-12-15T15:06:06.694153Z","shell.execute_reply.started":"2024-12-15T14:44:19.743266Z","shell.execute_reply":"2024-12-15T15:06:06.693357Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 21:44, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.267300</td>\n      <td>1.054682</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.200300</td>\n      <td>1.054211</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.192800</td>\n      <td>1.053403</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.164100</td>\n      <td>1.051566</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.203100</td>\n      <td>1.048601</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.246400</td>\n      <td>1.047036</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.115100</td>\n      <td>1.045370</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.215200</td>\n      <td>1.043112</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.234500</td>\n      <td>1.042529</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.130700</td>\n      <td>1.040197</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.309600</td>\n      <td>1.037183</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.100200</td>\n      <td>1.034491</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.194500</td>\n      <td>1.033551</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.138300</td>\n      <td>1.031451</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.149300</td>\n      <td>1.028509</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.215300</td>\n      <td>1.026983</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.063200</td>\n      <td>1.026196</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.093200</td>\n      <td>1.024151</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.158300</td>\n      <td>1.022226</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.126500</td>\n      <td>1.021033</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.140200</td>\n      <td>1.020438</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.063200</td>\n      <td>1.019804</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.142600</td>\n      <td>1.019032</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.125900</td>\n      <td>1.017741</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.160700</td>\n      <td>1.016566</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.041800</td>\n      <td>1.015356</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.119000</td>\n      <td>1.014754</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.193500</td>\n      <td>1.014188</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.146600</td>\n      <td>1.013314</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.050600</td>\n      <td>1.013064</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.094000</td>\n      <td>1.012117</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.213200</td>\n      <td>1.011146</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.186100</td>\n      <td>1.010220</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.115800</td>\n      <td>1.009363</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.136100</td>\n      <td>1.008928</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.220300</td>\n      <td>1.008498</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.227900</td>\n      <td>1.008088</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.140600</td>\n      <td>1.007860</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.131500</td>\n      <td>1.007542</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.090100</td>\n      <td>1.007199</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.129200</td>\n      <td>1.006960</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.133900</td>\n      <td>1.006791</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.083800</td>\n      <td>1.006670</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.048700</td>\n      <td>1.006559</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.091600</td>\n      <td>1.006512</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.180200</td>\n      <td>1.006499</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.061600</td>\n      <td>1.006504</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.113600</td>\n      <td>1.006504</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.150000</td>\n      <td>1.006501</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.126500</td>\n      <td>1.006501</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.1475318851470948, metrics={'train_runtime': 1306.4529, 'train_samples_per_second': 6.123, 'train_steps_per_second': 0.383, 'total_flos': 2087200751616000.0, 'train_loss': 1.1475318851470948, 'epoch': 2.824858757062147})"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuning_model_gpt2\")\ntokenizer.save_pretrained(\"./fine_tuning_model_gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:25:08.177377Z","iopub.execute_input":"2024-12-15T15:25:08.178183Z","iopub.status.idle":"2024-12-15T15:25:09.183136Z","shell.execute_reply.started":"2024-12-15T15:25:08.178132Z","shell.execute_reply":"2024-12-15T15:25:09.182281Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"('./fine_tuning_model_gpt2/tokenizer_config.json',\n './fine_tuning_model_gpt2/special_tokens_map.json',\n './fine_tuning_model_gpt2/vocab.json',\n './fine_tuning_model_gpt2/merges.txt',\n './fine_tuning_model_gpt2/added_tokens.json',\n './fine_tuning_model_gpt2/tokenizer.json')"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./fine_tuning_model_gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"./fine_tuning_model_gpt2\")\n\n\ninput_text = \"Write a function to calculate the factorial of a number.\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n\noutput = model.generate(\n    input_ids,\n    max_length=64,\n    num_return_sequences=3,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    top_k=10,\n    top_p=0.95,\n    do_sample=True,\n    early_stopping=True,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\nfor i, out in enumerate(output):\n    print(\"\\n\")\n    generated_text = tokenizer.decode(out, skip_special_tokens=True)\n    words = generated_text.split()\n\n\n    for j in range(0, len(words), 20):\n        line = ' '.join(words[j:j + 20])\n        print(line)\n    print(\"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:25:40.124794Z","iopub.execute_input":"2024-12-15T15:25:40.125564Z","iopub.status.idle":"2024-12-15T15:25:43.757926Z","shell.execute_reply.started":"2024-12-15T15:25:40.125533Z","shell.execute_reply":"2024-12-15T15:25:43.757011Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\n\nWrite a function to calculate the factorial of a number. function fact(n) { var fact = Math.floor(Math.random() * fact.exp(1)); }\nfunction fact2(fact){ var n = fact; var x = n * n; return fact * x; }\n\n\n\n\nWrite a function to calculate the factorial of a number. function fact_factorial(a, b) if a == b: return -1 else:\nprint(\"Number is \" + a + b + \"\") return 0 else : return 1 .def fact(n): for i in\nrange(\n\n\n\n\nWrite a function to calculate the factorial of a number. function fact(n, n) return fact * n . The fact\nfunction can be used to find the number of elements in a list. For example, if you have a string,\nyou can find it by using the string.find() function\n\n\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./fine_tuning_model_gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"./fine_tuning_model_gpt2\")\n\n\ninput_text = \"Write a function to calculate the factorial of a number.\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n\noutput = model.generate(\n    input_ids,\n    max_length=256,\n    num_return_sequences=3,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    top_k=10,\n    top_p=0.95,\n    do_sample=True,\n    early_stopping=True,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\nfor i, out in enumerate(output):\n    print(\"\\n\")\n    generated_text = tokenizer.decode(out, skip_special_tokens=True)\n    words = generated_text.split()\n\n\n    for j in range(0, len(words), 20):\n        line = ' '.join(words[j:j + 20])\n        print(line)\n    print(\"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:27:58.566278Z","iopub.execute_input":"2024-12-15T15:27:58.566639Z","iopub.status.idle":"2024-12-15T15:28:14.702964Z","shell.execute_reply.started":"2024-12-15T15:27:58.566610Z","shell.execute_reply":"2024-12-15T15:28:14.701910Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nWrite a function to calculate the factorial of a number. def fact(a, b): return a.fact(0) .sort(b) # sort by the\nnumber of elements in the list return fact The fact function returns the sum of the elements of all the\nnumbers in a list. It is a bit more complex than the above example, but it is still a good\nstarting point. The function is very simple to implement, and it can be implemented in Python as a class. This\nis also a great way to learn about factoring. In fact, the function calculates the square root of 2.0. So,\nit's not so much that it takes the same number as it does a fact number, because the two are\nnot the equal. But it gives you an idea of how factors work. For example: #!/usr/bin/env python import sys import\ntime def fact2(n): for i in range(1000): # find the first element in n of n if i == 0:\n# return the smallest element of ids ix = ices[i] if is(i+1): x = x + ichor(x, ics(1)) else: y\n\n\n\n\nWrite a function to calculate the factorial of a number. def fact_factorial(n): return n * fact .append(fact) The fact function\nis implemented by calling the function fact() on the number n. It returns the result of factoring the n factors\ninto the equation. The factor function returns a new value of n which is the sum of the factors. This\nvalue is then used to compute the factor(s) of that number, and the remainder is used as the constant. If\nn is greater than or equal to the given number of factors, the return value will be the same. Otherwise,\nit will return a negative number with the exception of one factor. >>> fact(1,2) = 1 >>> factors(2,3) >>> f(3,4)\n== 4 >>> return factores(4,5) ~~~~~~~~~~~~~~~~~~~~ (defn fact) ~~~~ ~~ The function fib_fib() is a simple function that takes a\nsingle number and returns an infinite list. The number is an arbitrary number that is not a prime number or\nprime root. You can use it to find the prime numbers in a list, or you can just\n\n\n\n\nWrite a function to calculate the factorial of a number. var fact = function(n) { return n < 1? fact[n]\n: n; }; var fact2 = fact.factorial(fact2); return fact1; . function fact3(a, b, c) var c = Math.floor(Math.random() * fact(1\n- a)); var a = a.sum(c); var b = b.toString(); var d = d.length === 0; return d[0] ? fact:\nfact; } The fact function is the same as the above function. It returns a fact number, and returns the\nsum of the two. The fact value is then a value that we can use to find the number n.\nThis function takes a string and a boolean and produces a result. If the string is a valid number then\nit will be a true number and if it is not then we will have to use a negation function\nwhich is similar to the negator function except that it returns true if the value has a length of 0.\nA negating function will return true when the input string contains an empty string. We can also use the function\nfact to check if we have a\n\n\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}